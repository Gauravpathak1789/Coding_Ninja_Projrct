{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ee0a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal,Annotated\n",
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import StateGraph,MessagesState,START,END\n",
    "from langgraph.types import Command\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Image\n",
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint\n",
    "from langchain_core.messages import HumanMessage,AIMessage,SystemMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from youtube_search import YoutubeSearch\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "import sqlite3\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph_supervisor import create_supervisor\n",
    "\n",
    "from langchain_community.utilities.semanticscholar import SemanticScholarAPIWrapper\n",
    "from langchain.schema import HumanMessage\n",
    "from typing import List, Dict\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled, VideoUnavailable\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "100d1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "YOUTUBE_API_KEY=os.getenv(\"YOUTUBE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "20b17096",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model=ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1c9bd005",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "871c1fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" Hello! It's nice to meet you. How can I assist you today? If you have any questions or need help with something, feel free to ask. I'm here to help!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 6, 'total_tokens': 47}, 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--871eb196-afb8-40d6-99ef-fae3abd871ff-0', usage_metadata={'input_tokens': 6, 'output_tokens': 41, 'total_tokens': 47})"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False,\n",
    "    top_p=1.0,\n",
    "    top_k=0,\n",
    "    provider=\"auto\"  # let Hugging Face choose the best provider for you\n",
    ")\n",
    "hf_model = ChatHuggingFace(llm=llm)\n",
    "hf_model.invoke([HumanMessage(content=\"Hello World\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4575fc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions (Request ID: Root=1-68baca15-0b94f3ee7172696b63362756;c4a48492-86eb-4ddc-8cc6-8efe4c682bc7)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Gaurav Pathak\\OneDrive\\Desktop\\Langgraph\\langvenv\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Gaurav Pathak\\OneDrive\\Desktop\\Langgraph\\langvenv\\lib\\site-packages\\requests\\models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFaceEndpoint(\n\u001b[0;32m      2\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/gpt-oss-20b\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# let Hugging Face choose the best provider for you\u001b[39;00m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m hf_model \u001b[38;5;241m=\u001b[39m ChatHuggingFace(llm\u001b[38;5;241m=\u001b[39mllm)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mhf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello World\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gaurav Pathak\\OneDrive\\Desktop\\Langgraph\\langvenv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:393\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    389\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    390\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 393\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    394\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    395\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    396\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    397\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    398\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    399\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    400\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    401\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    402\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    403\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\Gaurav Pathak\\OneDrive\\Desktop\\Langgraph\\langvenv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1019\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m   1012\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1017\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m   1018\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m-> 1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gaurav Pathak\\OneDrive\\Desktop\\Langgraph\\langvenv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:837\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    836\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 837\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    838\u001b[0m                 m,\n\u001b[0;32m    839\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    840\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    841\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    842\u001b[0m             )\n\u001b[0;32m    843\u001b[0m         )\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    845\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\Gaurav Pathak\\OneDrive\\Desktop\\Langgraph\\langvenv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1085\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1085\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1086\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1087\u001b[0m     )\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gaurav Pathak\\OneDrive\\Desktop\\Langgraph\\langvenv\\lib\\site-packages\\langchain_huggingface\\chat_models\\huggingface.py:577\u001b[0m, in \u001b[0;36mChatHuggingFace._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m     message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    571\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    576\u001b[0m     }\n\u001b[1;32m--> 577\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat_completion(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(answer)\n\u001b[0;32m    579\u001b[0m llm_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_chat_prompt(messages)\n",
      "File \u001b[1;32mc:\\Users\\Gaurav Pathak\\OneDrive\\Desktop\\Langgraph\\langvenv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:923\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[1;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[0;32m    895\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload_model,\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[0;32m    915\u001b[0m }\n\u001b[0;32m    916\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_request(\n\u001b[0;32m    917\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    918\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    921\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[0;32m    922\u001b[0m )\n\u001b[1;32m--> 923\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gaurav Pathak\\OneDrive\\Desktop\\Langgraph\\langvenv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:279\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[1;34m(self, request_parameters, stream)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\Gaurav Pathak\\OneDrive\\Desktop\\Langgraph\\langvenv\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions (Request ID: Root=1-68baca15-0b94f3ee7172696b63362756;c4a48492-86eb-4ddc-8cc6-8efe4c682bc7)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"openai/gpt-oss-20b\",\n",
    "#     task=\"text-generation\",\n",
    "#     do_sample=False,\n",
    "#     top_p=1.0,\n",
    "#     top_k=0,\n",
    "#     provider=\"auto\",  # let Hugging Face choose the best provider for you\n",
    "# )\n",
    "\n",
    "# hf_model = ChatHuggingFace(llm=llm)\n",
    "# hf_model.invoke([HumanMessage(content=\"Hello World\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5b99168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groq_model.invoke([HumanMessage(content=\"hii how are you?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbfc7e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Learning vs Deep Learning - https://www.youtube.com/watch?v=q6kJ71tEYqM\\nMachine Learning vs Deep Learning - https://www.youtube.com/watch?v=o3bWqPdWJ88\\nAI vs ML vs DL vs DS: What&#39;s the Difference? - https://www.youtube.com/watch?v=PopKlyqTPAI'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "@tool\n",
    "def youtube_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search YouTube for videos related to the query and return top 3 video links with titles.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    API_KEY = YOUTUBE_API_KEY\n",
    "    url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"q\": query,\n",
    "        \"type\": \"video\",\n",
    "        \"maxResults\": 3,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "    res = requests.get(url, params=params).json()\n",
    "    items = res.get(\"items\", [])\n",
    "    videos = [\n",
    "        f\"{item['snippet']['title']} - https://www.youtube.com/watch?v={item['id']['videoId']}\"\n",
    "        for item in items if item.get('id', {}).get('videoId')\n",
    "    ]\n",
    "    if not videos:\n",
    "        return \"No related videos found.\"\n",
    "    return \"\\n\".join(videos)\n",
    "\n",
    "youtube_search.invoke(\"difference between machine learning and deep learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7ea4618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyB4l5vmp8KFbpq4APojb4QvaT9u_Yyeix0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "print(API_KEY)  # Should print your API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4246c76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key is working!\n",
      "Channel title: Google for Developers\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "YOUTUBE_API_KEY=\"AIzaSyB4l5vmp8KFbpq4APojb4QvaT9u_Yyeix0\"\n",
    "API_KEY = YOUTUBE_API_KEY\n",
    "CHANNEL_ID = \"UC_x5XG1OV2P6uZZ5FSM9Ttw\"  # Example: Google Developers Channel\n",
    "\n",
    "url = f\"https://www.googleapis.com/youtube/v3/channels?part=snippet&id={CHANNEL_ID}&key={API_KEY}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(\"‚úÖ API key is working!\")\n",
    "    print(\"Channel title:\", data[\"items\"][0][\"snippet\"][\"title\"])\n",
    "else:\n",
    "    print(\"‚ùå API key NOT working! Status code:\", response.status_code)\n",
    "    print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17a6d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool\n",
    "def generate_resume(job_description: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a structured resume and cover letter based on the job description.\n",
    "    Returns output in clean, formatted HTML for easy display.\n",
    "    \"\"\"\n",
    "    \n",
    "    html_template = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; line-height: 1.5; }}\n",
    "            h2 {{ color: #2E86C1; }}\n",
    "            h3 {{ color: #117A65; }}\n",
    "            p {{ margin-bottom: 10px; }}\n",
    "            .section {{ margin-bottom: 20px; }}\n",
    "            .highlight {{ color: #D35400; font-weight: bold; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"section\">\n",
    "            <h2>üìÑ Resume - Tailored for: {job_description}</h2>\n",
    "            <h3>Summary</h3>\n",
    "            <p>Experienced professional with skills and expertise matching <span class=\"highlight\">{job_description}</span>. \n",
    "            Strong problem-solving abilities and a passion for excellence in the relevant field.</p>\n",
    "        </div>\n",
    "        <div class=\"section\">\n",
    "            <h3>Skills</h3>\n",
    "            <p>‚Ä¢ Core skills relevant to {job_description}<br>\n",
    "               ‚Ä¢ Additional complementary skills<br>\n",
    "               ‚Ä¢ Technical & soft skills</p>\n",
    "        </div>\n",
    "        <div class=\"section\">\n",
    "            <h3>Experience</h3>\n",
    "            <p>‚Ä¢ Previous roles and achievements tailored to {job_description}<br>\n",
    "               ‚Ä¢ Demonstrated impact and measurable results</p>\n",
    "        </div>\n",
    "        <div class=\"section\">\n",
    "            <h2>‚úâÔ∏è Cover Letter</h2>\n",
    "            <p>Dear Hiring Manager,</p>\n",
    "            <p>I am excited to apply for the <span class=\"highlight\">{job_description}</span> role. \n",
    "            With my background, skills, and passion for this field, I am confident in delivering impactful results. \n",
    "            I look forward to contributing to your team‚Äôs success.</p>\n",
    "            <p>Best regards,<br>Your Name</p>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return html_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6ab75d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SemanticScholarAPIWrapper(top_k_results=5, load_max_docs=5)\n",
    "\n",
    "@tool\n",
    "def semantic_scholar_research(query: str) -> List[Dict]:\n",
    "    \"\"\"Fetch and summarize top research papers from Semantic Scholar.\"\"\"\n",
    "    try:\n",
    "        raw = ss.run(query)\n",
    "        papers = raw.split(\"\\n\\n\")\n",
    "        result = []\n",
    "        \n",
    "        for p in papers[:3]:  # Limit to top 3 papers\n",
    "            if \"abstract:\" in p.lower():\n",
    "                summary = hf_model.invoke([\n",
    "                    HumanMessage(content=f\"Summarize this research paper in 3-4 sentences:\\n{p}\")\n",
    "                ]).content\n",
    "                result.append({\"raw_info\": p, \"summary\": summary})\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return [{\"error\": f\"Error fetching papers: {str(e)}\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dbc193d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'raw_info': 'Published year: 2023\\nTitle: One Hundred Most-cited Papers on Bacterial Meningitis: A Bibliometric Study\\nAuthors: Vinayak P Hakkaraki\\nAbstract: Background: In previous decades, large-scale research has been carried out on bacterial meningitis. In every field, citation analysis is the most significant contribution. The study‚Äôs objective was to identify and analyze the 100 articles on bacterial meningitis that received the most citations between 2000 and 2023, highlighting the most significant developments in the field. Objective: The objective of this study was to find out what makes a highly influential article by identifying and analyzing the characteristics of the 100 articles in the field of bacterial meningitis that receive the most citations. The goal of this study was to find and examine the 100 articles on bacterial meningitis that received the most citations. Methodology: We identified the top 100 most-cited papers in the field of bacterial meningitis from 55 journals using the Dimensions AI database. The results of each author‚Äôs analysis of 100 articles were then compared. We gathered fundamental data such as the journal‚Äôs title, country of publication, and study type. Descriptive counts or percentages were used to compare the various categories. Results: Between the year 2000 and the year 2023, articles were published. The total number of citations ranged from 115 to 1176, with 42 papers receiving more than 200 citations. In 2008, 14 articles were published, followed by 10 in 2000 and 2007. One thousand one hundred and seventy-six times were given to the most-cited paper, whereas 115 times were given to the least-cited article. ‚ÄúClinical Features and Prognostic Factors in Adults with Bacterial Meningitis,‚Äù by Diederik van de Beek, et al. (2004) was the article that received the most citations. 1176 people have cited this article. van de Beek Diederik of the Academic Medical Center in The Netherlands is the author who has written the most articles, was mentioned in 14 of the top 100 articles. Papers were primarily published in Pediatrics (n = 9) publication with 1861 citations. The Netherlands came in second with 18 publications, followed by the United States (n = 46). Conclusion: Our study uses bibliometrics and visualization analysis of the most important articles in this field to show the current state of research in the area of bacterial meningitis, provide a history of research trends, and offer a perspective for future bacterial predicts the growth of meningitis.',\n",
       "  'summary': ' This 2023 research paper, titled \"One Hundred Most-cited Papers on Bacterial Meningitis: A Bibliometric Study,\" conducted by Vinayak P Hakkaraki, aims to identify and analyze the 100 articles on bacterial meningitis that received the most citations between 2000 and 2023. The study found that the most-cited paper, published in 2004 by Diederik van de Beek et al., received 1176 citations. The Netherlands and the United States were the top two countries with the most publications in this field, and Pediatrics was the journal with the most publications and citations. The study provides insights into the current state of research, trends, and potential future directions in the field of bacterial meningitis.'},\n",
       " {'raw_info': '\\nPublished year: 2023\\nTitle: Scientometric assessment of funded scientometrics and bibliometrics research (2011‚Äì2021)\\nAuthors: M. Verma, Daud Khan, M. Yuvaraj\\nAbstract: None',\n",
       "  'summary': ' The 2023 research paper titled \"Scientometric assessment of funded scientometrics and bibliometrics research (2011‚Äì2021)\" by M. Verma, Daud Khan, and M. Yuvaraj analyzes the funding trends, productivity, and impact of research in the field of scientometrics and bibliometrics from 2011 to 2021. The study identifies key countries, institutions, and researchers contributing significantly to this field, and provides insights into the evolution and future directions of funded research in scientometrics and bibliometrics.'},\n",
       " {'raw_info': '\\nPublished year: 2023\\nTitle: Exploring the status of artificial intelligence for healthcare research in Africa: a bibliometric and thematic analysis\\nAuthors: Tabu S. Kondo, Salim Diwani, Ally S. Nyamawe, Mohamedi M. Mjahidi\\nAbstract: This paper explores the status of Artificial Intelligence (AI) for healthcare research in Africa. The aim was to use bibliometric and thematic analysis methods to determine the publication counts, leading authors, top journals and publishers, most active institutions and countries, most cited institutions, funding bodies, top subject areas, co-occurrence of keywords and co-authorship. Bibliographic data were collected on April 9 2022, through the Lens database, based on the critical areas of authorship studies, such as authorship pattern, number of authors, etc. The findings showed that several channels were used to disseminate the publications, including articles, conference papers, reviews, and others. Publications on computer science topped the list of documented subject categories. The Annals of Tropical Medicine and Public Health is the top journal, where articles on AI have been published. One of the top nations that published AI research was the United Kingdom. With 143 publications, Harvard University was the higher education institution that p',\n",
       "  'summary': ' The 2023 research paper titled \"Exploring the status of artificial intelligence for healthcare research in Africa\" analyzes AI usage in healthcare research across Africa. Using bibliometric and thematic analysis, the study identifies key trends such as leading authors, top journals, active institutions, and countries, most cited institutions, funding bodies, subject areas, and co-occurrence of keywords. The findings indicate that computer science publications lead the subject categories, Annals of Tropical Medicine and Public Health is the top journal, the United Kingdom is among the top publishing nations, and Harvard University is the most active higher education institution in this field.'}]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_scholar_research(\"top AI research papers 2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84f6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Prompt Template\n",
    "# -----------------------------\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant designed to answer questions about a YouTube video link taht have been provided based on its transcript.\n",
    "Answer the user's question using ONLY the provided transcript context.\n",
    "If the information is not in the context, explicitly say \"I cannot find information about that in the video transcript.\"\n",
    "\n",
    "Transcript:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
    "\n",
    "# -----------------------------\n",
    "# Transcript Fetcher\n",
    "# -----------------------------\n",
    "def get_transcript(video_id: str):\n",
    "    \"\"\"Fetch transcript safely for old and new versions of youtube_transcript_api.\"\"\"\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi().fetch(video_id, languages=[\"hi\"])\n",
    "    except NoTranscriptFound:\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi().fetch(video_id, languages=[\"en\"])\n",
    "        except NoTranscriptFound:\n",
    "            return None, \"No transcript available in Hindi or English.\"\n",
    "    except (TranscriptsDisabled, VideoUnavailable) as e:\n",
    "        return None, str(e)\n",
    "    except Exception as e:\n",
    "        return None, f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "    # Handle both dicts and FetchedTranscriptSnippet objects\n",
    "    texts = []\n",
    "    for snippet in transcript_list:\n",
    "        if isinstance(snippet, dict):\n",
    "            texts.append(snippet.get(\"text\", \"\"))\n",
    "        else:\n",
    "            # FetchedTranscriptSnippet object\n",
    "            texts.append(getattr(snippet, \"text\", \"\"))\n",
    "\n",
    "    transcript = \" \".join(texts)\n",
    "    return transcript, None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main YouTube QA / Summarizer\n",
    "# -----------------------------\n",
    "@tool\n",
    "def youtube_qa(video_url: str, question: str):\n",
    "    \"\"\"Given a YouTube URL and question, return answer based on transcript.\"\"\"\n",
    "    try:\n",
    "        video_id = video_url.split(\"v=\")[-1].split(\"&\")[0]  # Extract only the video ID\n",
    "        transcript, error = get_transcript(video_id)\n",
    "        if error:\n",
    "            return error\n",
    "\n",
    "        rag_runnable = (\n",
    "            {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | hf_model\n",
    "        )\n",
    "        answer = rag_runnable.invoke({\"context\": transcript, \"question\": question})\n",
    "        return answer\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d017872",
   "metadata": {},
   "outputs": [],
   "source": [
    "duck_tool = DuckDuckGoSearchRun()\n",
    "tavile_tool = TavilySearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "18ff3380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a resume and cover letter for a Software Engineer internship\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Explanation:\n",
      "Creating a resume and cover letter for a Software Engineer internship involves highlighting your technical skills, programming knowledge, and any relevant projects or experiences. Your resume should include sections such as an objective or summary, education, technical skills, projects, and any internships or work experience. The cover letter should be tailored to the specific internship, expressing your interest in the role and how your skills align with the company's needs. Both documents should be concise, well-formatted, and free of errors.\n",
      "\n",
      "YouTube Videos:\n",
      "1. How to Write a Software Engineering Resume - CareerCounselors - https://www.youtube.com/watch?v=6Kl9G9Y3MwI\n",
      "2. Cover Letter Tips for Software Engineers | How to Write a Cover Letter - Indeed Career Tips - https://www.youtube.com/watch?v=ZXC4Pj7WS4\n",
      "3. How to Write a Cover Letter for a Software Engineering Internship (Example Included) - Code With Mosh - https://www.youtube.com/watch?v=3jg9g4K7_70\n",
      "\n",
      "Learning Steps (optional):\n",
      "1. Review the explanation and understand the key components of a resume and cover letter for a Software Engineer internship.\n",
      "2. Use the YouTube videos to get practical tips and examples for writing your resume and cover letter.\n",
      "3. Apply the knowledge by drafting your resume and cover letter, ensuring they are tailored to the specific internship you are applying for.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Network-Agent Orchestration of Your Multi-Agent System (LLM-classified router)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "from typing import List, Dict\n",
    "import re\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# LangChain / LangGraph\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.types import Command\n",
    "\n",
    "# Tools / APIs\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_community.utilities.semanticscholar import SemanticScholarAPIWrapper\n",
    "\n",
    "from youtube_transcript_api import (\n",
    "    YouTubeTranscriptApi,\n",
    "    NoTranscriptFound,\n",
    "    TranscriptsDisabled,\n",
    "    VideoUnavailable,\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Env & Models\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Optional: Groq model (not required by router, but kept as in your code)\n",
    "hf_model = ChatGroq(model=\"deepseek-r1-distill-llama-70b\")\n",
    "\n",
    "# Primary chat model (HF)\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "#     task=\"text-generation\",\n",
    "#     do_sample=False,\n",
    "#     top_p=1.0,\n",
    "#     top_k=0,\n",
    "#     provider=\"auto\",\n",
    "# )\n",
    "# hf_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Quick sanity ping (can be commented out)\n",
    "_ = hf_model.invoke([HumanMessage(content=\"Hello World\")])\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Tools\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# @tool\n",
    "# def youtube_search(query: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Search YouTube for videos related to the query and return top 3 video links with titles.\n",
    "#     \"\"\"\n",
    "#     API_KEY = \"AIzaSyBNBTgze_5FR5VHzfZlxc38iLwr7xyYaHE\"  # (kept as given)\n",
    "#     url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "#     params = {\n",
    "#         \"part\": \"snippet\",\n",
    "#         \"q\": query,\n",
    "#         \"type\": \"video\",\n",
    "#         \"maxResults\": 3,\n",
    "#         \"key\": API_KEY,\n",
    "#     }\n",
    "#     res = requests.get(url, params=params).json()\n",
    "#     items = res.get(\"items\", [])\n",
    "#     videos = [\n",
    "#         f\"{item['snippet']['title']} - https://www.youtube.com/watch?v={item['id']['videoId']}\"\n",
    "#         for item in items\n",
    "#         if item.get(\"id\", {}).get(\"videoId\")\n",
    "#     ]\n",
    "#     if not videos:\n",
    "#         return \"No related videos found.\"\n",
    "#     return \"\\n\".join(videos)\n",
    "# \n",
    "API_KEY = \"AIzaSyBNBTgze_5FR5VHzfZlxc38iLwr7xyYaHE\"\n",
    "\n",
    "def youtube_search_guaranteed(query: str, max_results=3):\n",
    "    # Step 1: search videos\n",
    "    search_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"q\": query,\n",
    "        \"type\": \"video\",\n",
    "        \"maxResults\": 20,  # search more for filtering\n",
    "        \"key\": API_KEY,\n",
    "    }\n",
    "    search_res = requests.get(search_url, params=params).json()\n",
    "    items = search_res.get(\"items\", [])\n",
    "    video_ids = [item['id']['videoId'] for item in items if item.get('id', {}).get('videoId')]\n",
    "    if not video_ids:\n",
    "        return []\n",
    "\n",
    "    # Step 2: get video details to filter valid ones\n",
    "    details_url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "    params = {\n",
    "        \"part\": \"status,snippet\",\n",
    "        \"id\": \",\".join(video_ids),\n",
    "        \"key\": API_KEY,\n",
    "    }\n",
    "    details_res = requests.get(details_url, params=params).json()\n",
    "    valid_videos = []\n",
    "\n",
    "    for video in details_res.get(\"items\", []):\n",
    "        status = video.get(\"status\", {})\n",
    "        if status.get(\"uploadStatus\") != \"processed\":\n",
    "            continue\n",
    "        if status.get(\"privacyStatus\") != \"public\":\n",
    "            continue\n",
    "        snippet = video.get(\"snippet\", {})\n",
    "        title = snippet.get(\"title\")\n",
    "        vid_id = video.get(\"id\")\n",
    "        link = f\"https://www.youtube.com/watch?v={vid_id}\"\n",
    "        valid_videos.append(f\"{title} - {link}\")\n",
    "        if len(valid_videos) >= max_results:\n",
    "            break\n",
    "\n",
    "    return valid_videos\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ LLM-based Topic Extractor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "\n",
    "@tool\n",
    "def youtube_search_agent(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Deterministically extract main keywords and fetch top 3 valid YouTube videos.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    # Step 1: Extract main topic with simple heuristic (first few nouns / keywords)\n",
    "    # For example, remove \"Explain\", \"Give me\", etc.\n",
    "    cleaned_query = re.sub(\n",
    "        r\"\\b(explain|give me|show|video|tutorial|about|for|the|a|an)\\b\", \"\", query, flags=re.I\n",
    "    )\n",
    "    # Keep only the first 5‚Äì7 words\n",
    "    keywords = \" \".join(cleaned_query.split()[:7])\n",
    "\n",
    "    # Step 2: Search YouTube\n",
    "    videos = youtube_search_guaranteed(keywords)\n",
    "    if not videos:\n",
    "        return \"No valid YouTube videos found for this query.\"\n",
    "    return \"\\n\".join(videos)\n",
    "\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Topic Explanation Tool ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "@tool\n",
    "def topic_explanation(query: str) -> str:\n",
    "    \"\"\" Return a concise conceptual explanation of the topic. \"\"\"\n",
    "    prompt = f\"Explain the concept of '{query}' in 3-5 sentences in a clear, beginner-friendly way.\"\n",
    "    return hf_model.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "\n",
    "@tool\n",
    "def generate_resume(job_description: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a structured resume and cover letter based on the job description.\n",
    "    Returns output in clean, formatted HTML for easy display.\n",
    "    \"\"\"\n",
    "    html_template = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; line-height: 1.5; }}\n",
    "            h2 {{ color: #2E86C1; }}\n",
    "            h3 {{ color: #117A65; }}\n",
    "            p {{ margin-bottom: 10px; }}\n",
    "            .section {{ margin-bottom: 20px; }}\n",
    "            .highlight {{ color: #D35400; font-weight: bold; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"section\">\n",
    "            <h2>üìÑ Resume - Tailored for: {job_description}</h2>\n",
    "            <h3>Summary</h3>\n",
    "            <p>Experienced professional with skills and expertise matching <span class=\"highlight\">{job_description}</span>. \n",
    "            Strong problem-solving abilities and a passion for excellence in the relevant field.</p>\n",
    "        </div>\n",
    "        <div class=\"section\">\n",
    "            <h3>Skills</h3>\n",
    "            <p>‚Ä¢ Core skills relevant to {job_description}<br>\n",
    "               ‚Ä¢ Additional complementary skills<br>\n",
    "               ‚Ä¢ Technical & soft skills</p>\n",
    "        </div>\n",
    "        <div class=\"section\">\n",
    "            <h3>Experience</h3>\n",
    "            <p>‚Ä¢ Previous roles and achievements tailored to {job_description}<br>\n",
    "               ‚Ä¢ Demonstrated impact and measurable results</p>\n",
    "        </div>\n",
    "        <div class=\"section\">\n",
    "            <h2>‚úâÔ∏è Cover Letter</h2>\n",
    "            <p>Dear Hiring Manager,</p>\n",
    "            <p>I am excited to apply for the <span class=\"highlight\">{job_description}</span> role. \n",
    "            With my background, skills, and passion for this field, I am confident in delivering impactful results. \n",
    "            I look forward to contributing to your team‚Äôs success.</p>\n",
    "            <p>Best regards,<br>Your Name</p>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return html_template\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(url: str, max_pages: int = 5) -> str:\n",
    "    try:\n",
    "        response = requests.get(url, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            return \"\"\n",
    "        pdf_file = BytesIO(response.content)\n",
    "        reader = PdfReader(pdf_file)\n",
    "        text = [page.extract_text() or \"\" for page in reader.pages[:max_pages]]\n",
    "        return \"\\n\".join(text)\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting PDF: {e}\"\n",
    "\n",
    "\n",
    "# Semantic Scholar wrapper\n",
    "ss = SemanticScholarAPIWrapper(\n",
    "    top_k_results=5,\n",
    "    load_max_docs=5\n",
    ")\n",
    "\n",
    "@tool\n",
    "def semantic_scholar_research(query: str, summarize: bool = True) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch top research papers from Semantic Scholar for a query.\n",
    "    - Optionally summarize abstracts using LLM.\n",
    "    - Attempt PDF extraction if URL is provided.\n",
    "    \"\"\"\n",
    "    raw_results = ss.run(query)  # returns string with paper info (lib‚Äôs format)\n",
    "    papers = raw_results.split(\"\\n\\n\")\n",
    "    summarized_papers = []\n",
    "\n",
    "    for paper in papers:\n",
    "        summary = \"\"\n",
    "        pdf_summary = \"\"\n",
    "        if summarize and \"abstract:\" in paper.lower():\n",
    "            prompt = f\"Summarize this research abstract in 2-3 sentences:\\n\\n{paper}\"\n",
    "            summary = hf_model.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "        if \"http\" in paper and \".pdf\" in paper.lower():\n",
    "            pdf_url = \"http\" + paper.split(\"http\")[1].split()[0]\n",
    "            pdf_text = extract_text_from_pdf(pdf_url)\n",
    "            if pdf_text and summarize:\n",
    "                pdf_prompt = f\"Summarize this PDF content (first 500 words):\\n\\n{pdf_text[:3000]}\"\n",
    "                pdf_summary = hf_model.invoke([HumanMessage(content=pdf_prompt)]).content\n",
    "\n",
    "        summarized_papers.append(\n",
    "            {\n",
    "                \"raw_info\": paper,\n",
    "                \"summary\": summary,\n",
    "                \"pdf_summary\": pdf_summary,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return summarized_papers\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# YouTube QA Tooling\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "yt_prompt_template = \"\"\"\n",
    "You are a helpful assistant designed to answer questions about a YouTube video based on its transcript.\n",
    "Answer the user's question using ONLY the provided transcript context.\n",
    "If the information is not in the context, explicitly say \"I cannot find information about that in the video transcript.\"\n",
    "\n",
    "Transcript:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "yt_prompt = PromptTemplate(\n",
    "    template=yt_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def get_transcript(video_id: str):\n",
    "    \"\"\"Fetch transcript safely for old and new versions of youtube_transcript_api.\"\"\"\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi().fetch(video_id, languages=[\"hi\"])\n",
    "    except NoTranscriptFound:\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi().fetch(video_id, languages=[\"en\"])\n",
    "        except NoTranscriptFound:\n",
    "            return None, \"No transcript available in Hindi or English.\"\n",
    "    except (TranscriptsDisabled, VideoUnavailable) as e:\n",
    "        return None, str(e)\n",
    "    except Exception as e:\n",
    "        return None, f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "    texts = []\n",
    "    for snippet in transcript_list:\n",
    "        if isinstance(snippet, dict):\n",
    "            texts.append(snippet.get(\"text\", \"\"))\n",
    "        else:\n",
    "            texts.append(getattr(snippet, \"text\", \"\"))\n",
    "    transcript = \" \".join(texts)\n",
    "    return transcript, None\n",
    "\n",
    "\n",
    "@tool\n",
    "def youtube_qa(video_url: str, question: str):\n",
    "    \"\"\"Given a YouTube URL and question, return answer based on transcript.\"\"\"\n",
    "    try:\n",
    "        video_id = video_url.split(\"v=\")[-1].split(\"&\")[0]\n",
    "        transcript, error = get_transcript(video_id)\n",
    "        if error:\n",
    "            return error\n",
    "\n",
    "        rag_runnable = (\n",
    "            {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "            | yt_prompt\n",
    "            | hf_model\n",
    "        )\n",
    "        answer = rag_runnable.invoke({\"context\": transcript, \"question\": question})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# External Search Tools\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "duck_tool = DuckDuckGoSearchRun()\n",
    "tavily_tool = TavilySearch()\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Agents (unchanged behavior, same prompts & tools)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "unified_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[youtube_qa, semantic_scholar_research],\n",
    "    name=\"unified_agent\",\n",
    "    prompt=\"\"\"\n",
    "You are UNIFIED-AGENT, capable of handling YouTube transcript QA and research paper summarization.\n",
    "- If a valid YouTube link is provided, use youtube_qa to answer or summarize the transcript.\n",
    "- If a research query or PDF link is provided, use semantic_scholar_research to fetch and summarize papers.\n",
    "- Indicate the source clearly (video or research paper).\n",
    "- If info is unavailable, say so explicitly.\n",
    "- Always provide concise, clear, and user-friendly answers.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# @tool\n",
    "# def topic_explanation(query: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Return a concise conceptual explanation of the topic.\n",
    "#     \"\"\"\n",
    "#     prompt = f\"Explain the concept of '{query}' in 3-5 sentences in a clear, beginner-friendly way.\"\n",
    "#     return hf_model.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "# Now the curriculum agent only orchestrates:\n",
    "# curriculum_agent = create_react_agent(\n",
    "#     model=hf_model,\n",
    "#     tools=[duck_tool, youtube_search, topic_explanation],\n",
    "#     name=\"curriculum_agent\",\n",
    "#     prompt=\"\"\"\n",
    "# You are CURRICULUM-GUIDE, a knowledgeable educational expert.\n",
    "\n",
    "# Instructions:\n",
    "# 1. Use the `topic_explanation` tool to generate a clear textual explanation of the user's query.\n",
    "# 2. Use the `youtube_search` tool to fetch **real, top 3 YouTube video links** for the topic.\n",
    "# 3. Provide optional learning steps after explanation and videos.\n",
    "# 4. Format output exactly like this:\n",
    "\n",
    "# Explanation:\n",
    "# [Use topic_explanation output] and if link is not find then not return this type somethng below-' [Exact URL i]'\n",
    "\n",
    "# YouTube Videos:\n",
    "# 1. [Exact title 1] - [Exact URL 1]\n",
    "# 2. [Exact title 2] - [Exact URL 2]\n",
    "# 3. [Exact title 3] - [Exact URL 3]\n",
    "\n",
    "# Learning Steps (optional):\n",
    "# 1. Step one\n",
    "# 2. Step two\n",
    "# 3. Step three\n",
    "\n",
    "# Always:\n",
    "# - Use **actual video links** returned by `youtube_search`, never invented links.\n",
    "# - Keep explanation and steps concise, clear, and informative.\n",
    "# \"\"\"\n",
    "# )\n",
    "curriculum_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[topic_explanation, youtube_search_agent],\n",
    "    name=\"curriculum_agent\",\n",
    "    prompt=\"\"\"\n",
    "You are CURRICULUM-GUIDE, a knowledgeable educational expert.\n",
    "\n",
    "Instructions:\n",
    "1. Use topic_explanation to generate a clear explanation.\n",
    "2. Use youtube_search_agent to fetch **top 3 valid YouTube videos**.\n",
    "3. Provide optional learning steps after explanation.\n",
    "4. Never hallucinate video links. If youtube_search_agent returns empty, skip the YouTube section.\n",
    "\n",
    "Format output like this:\n",
    "\n",
    "Explanation:\n",
    "[Explanation text]\n",
    "\n",
    "YouTube Videos:\n",
    "1. [Video 1 title] - [Video 1 URL]\n",
    "2. [Video 2 title] - [Video 2 URL]\n",
    "3. [Video 3 title] - [Video 3 URL]\n",
    "\n",
    "Learning Steps (optional):\n",
    "1. Step one\n",
    "2. Step two\n",
    "3. Step three\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "debug_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[youtube_search, duck_tool, tavily_tool],\n",
    "    name=\"debug_educational_agent\",\n",
    "    prompt=\"\"\"\n",
    "You are CODE-TUTOR, a friendly and knowledgeable AI teacher and coding mentor. \n",
    "When a student asks a question or submits code, follow these rules:\n",
    "\n",
    "1. **Explain concepts clearly** in an educational and beginner-friendly manner.\n",
    "2. **Debug the code**: identify errors, explain why they occur, and suggest precise fixes.\n",
    "3. **Provide learning resources**: include relevant YouTube links or tutorials . \n",
    "4. **Give examples, exercises, or mini-quizzes** when helpful to reinforce understanding.\n",
    "5. **Motivate the student**: encourage practice, exploration, and curiosity.\n",
    "6. **Step-by-step guidance**: never give one-line answers; always walk the student through solutions.\n",
    "7. **Format output clearly**: use numbered steps, bullet points, or code blocks where applicable.\n",
    "\n",
    "Goal: Deliver comprehensive, interactive, and motivational educational guidance with youtube video.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "resume_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[generate_resume],\n",
    "    name=\"resume_agent\",\n",
    "    prompt=\"\"\"\n",
    "You are RESUME-GUIDE, an expert in generating resumes and cover letters. Your task is to produce\n",
    "a high-quality, professional, structured resume and cover letter for the given job description.\n",
    "\n",
    "Rules:\n",
    "1. Use the provided generate_resume tool for producing HTML output.\n",
    "2. Tailor content dynamically to match the job description.\n",
    "3. Ensure output is balanced: concise yet informative, highlighting key skills, achievements, and fit.\n",
    "4. Internally iterate if needed to refine clarity, professionalism, and formatting.\n",
    "5. Return only the **final polished HTML output** without any internal messages.\n",
    "\n",
    "Sections to include:\n",
    "- Summary\n",
    "- Skills\n",
    "- Experience\n",
    "- Cover Letter\n",
    "\n",
    "Goal: Generate a professional, polished, and ready-to-use resume + cover letter.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "news_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[tavily_tool, duck_tool, youtube_search],\n",
    "    name=\"news_agent\",\n",
    "    prompt=\"\"\"\n",
    "You are TECH-TREND-GUIDE, a proactive and knowledgeable technology analyst. Your task:\n",
    "\n",
    "1. Fetch the latest and trending technology news from multiple sources dynamically.\n",
    "2. Summarize each news point clearly in **point-wise format** for quick reading.\n",
    "3. Highlight **emerging tools, frameworks, and technologies** mentioned or relevant.\n",
    "4. Include **YouTube videos** if they are educational, relevant, and verified. If no video is available, skip gracefully.\n",
    "5. Provide **actionable insights or recommendations** wherever possible.\n",
    "6. Decide intelligently which tool (tavily_tool, duck_tool, or youtube_search) to use for each query.\n",
    "7. Keep the summary **concise, engaging, and readable**; avoid overly long paragraphs.\n",
    "8. Return **only the final summarized news**, do not include internal reasoning or tool calls.\n",
    "\n",
    "Example output format:\n",
    "- News Point 1: [Brief summary]  \n",
    "  Video (if available): [YouTube link]  \n",
    "- News Point 2: [Brief summary]  \n",
    "  Video (if available): [YouTube link]  \n",
    "- Trending Tools/Tech: [List of tools or technologies]  \n",
    "- Recommendation: [Optional actionable insight]\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# LLM-based Intent Classifier for Routing\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "_ALLOWED_INTENTS = [\"curriculum\", \"debug\", \"resume\", \"unified\", \"news\"]\n",
    "\n",
    "_CLASSIFY_SYSTEM = SystemMessage(\n",
    "    content=(\n",
    "        \"You are a router that classifies user messages into one of these intents:\\n\"\n",
    "        f\"{', '.join(_ALLOWED_INTENTS)}.\\n\"\n",
    "        \"Return ONLY the intent name (single lowercase word). No punctuation, no extra text.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "def classify_intent(user_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Dynamically classify user queries to the correct agent using the HF model.\n",
    "    No manual keyword heuristics. Fully context-aware.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a dynamic intent classifier for a multi-agent system.\n",
    "You must determine which agent should handle the user's query.\n",
    "\n",
    "Agents:\n",
    "1. curriculum - for learning guides, roadmaps, concept explanations, tutorials.\n",
    "2. debug - for code debugging, explaining errors, improving code, analyzing programming queries.\n",
    "3. resume - for generating resumes, cover letters, CVs.\n",
    "4. unified - for research papers, academic queries, YouTube video Q&A, and content summarization.\n",
    "5. news - for technology news, trends, updates, or alerts.\n",
    "\n",
    "Instructions:\n",
    "- Analyze the user's query carefully and select the **most appropriate agent**.\n",
    "- Consider the **purpose of the query**, not just keywords. E.g., 'review my resume' should go to debug if feedback is needed.\n",
    "- Return **ONLY** the agent name: curriculum, debug, resume, unified, or news. No punctuation, no extra text.\n",
    "- Be fully dynamic and context-aware. Do not fall back to static rules.\n",
    "\n",
    "User Query:\n",
    "\\\"\\\"\\\"{user_text}\\\"\\\"\\\"\n",
    "\n",
    "Agent:\n",
    "\"\"\"\n",
    "\n",
    "    msg = HumanMessage(content=prompt)\n",
    "    try:\n",
    "        intent = hf_model.invoke([_CLASSIFY_SYSTEM, msg]).content.strip().lower()\n",
    "        # Ensure it matches allowed intents\n",
    "        if intent in _ALLOWED_INTENTS:\n",
    "            return intent\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback to 'curriculum' safely\n",
    "    return \"curriculum\"\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Router Node + Graph (Network Pattern)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# We‚Äôll use MessagesState so messages flow naturally between nodes.\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "def router_node(state: MessagesState) -> MessagesState:\n",
    "    \"\"\"No-op node; conditional edges below decide the next hop.\"\"\"\n",
    "    return state\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_youtube_url(text: str) -> str:\n",
    "    \"\"\"Extract first YouTube URL from the text, or return None.\"\"\"\n",
    "    match = re.search(r\"(https?://www\\.youtube\\.com/watch\\?v=[\\w-]+)\", text)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "def preprocess_message_for_unified(message: str) -> str:\n",
    "    url = extract_youtube_url(message)\n",
    "    if url:\n",
    "        question = message.replace(url, \"\").strip()\n",
    "        # Format as structured call for youtube_qa\n",
    "        return f\"youtube_qa video_url={url} question={question}\"\n",
    "    return message\n",
    "\n",
    "\n",
    "def route_choice(state: MessagesState) -> str:\n",
    "    last_user = \"\"\n",
    "    for m in reversed(state[\"messages\"]):\n",
    "        if getattr(m, \"type\", None) == \"human\" or getattr(m, \"role\", \"\") == \"user\":\n",
    "            last_user = m.content if hasattr(m, \"content\") else (m.get(\"content\", \"\") if isinstance(m, dict) else \"\")\n",
    "            break\n",
    "\n",
    "    intent = classify_intent(last_user)\n",
    "    mapping = {\n",
    "        \"curriculum\": \"curriculum_agent\",\n",
    "        \"debug\": \"debug_agent\",\n",
    "        \"resume\": \"resume_agent\",\n",
    "        \"unified\": \"unified_agent\",\n",
    "        \"news\": \"news_agent\",\n",
    "    }\n",
    "\n",
    "    if intent == \"unified\":\n",
    "        # Preprocess only for unified\n",
    "        last_user = preprocess_message_for_unified(last_user)\n",
    "        state[\"messages\"].append(HumanMessage(content=last_user))\n",
    "\n",
    "    # For resume, curriculum, debug, news -> do nothing extra\n",
    "    return mapping.get(intent, \"curriculum_agent\")\n",
    "\n",
    "\n",
    "# Nodes\n",
    "builder.add_node(\"router\", router_node)\n",
    "\n",
    "# Agent nodes (callables from create_react_agent)\n",
    "builder.add_node(\"curriculum_agent\", curriculum_agent)\n",
    "builder.add_node(\"debug_agent\", debug_agent)\n",
    "builder.add_node(\"resume_agent\", resume_agent)\n",
    "builder.add_node(\"unified_agent\", unified_agent)\n",
    "builder.add_node(\"news_agent\", news_agent)\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"router\")\n",
    "builder.add_conditional_edges(\n",
    "    \"router\",\n",
    "    route_choice,\n",
    "    {\n",
    "        \"curriculum_agent\": \"curriculum_agent\",\n",
    "        \"debug_agent\": \"debug_agent\",\n",
    "        \"resume_agent\": \"resume_agent\",\n",
    "        \"unified_agent\": \"unified_agent\",\n",
    "        \"news_agent\": \"news_agent\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# End after each agent completes\n",
    "builder.add_edge(\"curriculum_agent\", END)\n",
    "builder.add_edge(\"debug_agent\", END)\n",
    "builder.add_edge(\"resume_agent\", END)\n",
    "builder.add_edge(\"unified_agent\", END)\n",
    "builder.add_edge(\"news_agent\", END)\n",
    "\n",
    "# Compile app\n",
    "network_app = builder.compile()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Example: invoke\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: curriculum-style query\n",
    "    q1 = \"Create a resume and cover letter for a Software Engineer internship\"\n",
    "    result1 = network_app.invoke({\"messages\": [{\"role\": \"user\", \"content\": q1}]})\n",
    "    for m in result1[\"messages\"]:\n",
    "        if hasattr(m, \"content\"):\n",
    "            print(m.content)\n",
    "            print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab0d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7cd9ef71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None:\n",
      "Summarize the paper 'Query-Relevant Summarization using FAQs'; Difference between CNN and RNN and show related YouTube video.\n",
      "================================================================================\n",
      "\n",
      "curriculum_agent:\n",
      " Title: Summary of 'Query-Relevant Summarization using FAQs'\n",
      "\n",
      "The paper 'Query-Relevant Summarization using FAQs' proposes a method for generating summaries that are more relevant to a user's query by utilizing Frequently Asked Questions (FAQs). The approach involves extracting key phrases from the FAQs and using them to guide the summarization process. This technique aims to improve the quality and relevance of summaries, making them more useful for users.\n",
      "\n",
      "Topic Explanation:\n",
      "\n",
      "Query-Relevant Summarization: This is a summarization technique that focuses on creating summaries that are directly relevant to a user's query. The goal is to provide concise and accurate information that answers the user's question or addresses their information need.\n",
      "\n",
      "Frequently Asked Questions (FAQs): FAQs are a collection of common questions and answers related to a specific topic or product. They are often used to help users find information quickly and easily.\n",
      "\n",
      "Difference between CNN and RNN:\n",
      "\n",
      "Convolutional Neural Networks (CNN): CNNs are a type of neural network commonly used for image processing tasks. They use convolutional layers to automatically and adaptively learn spatial hierarchies of features from the input data.\n",
      "\n",
      "Recurrent Neural Networks (RNN): RNNs are a type of neural network that can process sequential data. They have loops in their architecture, allowing information to be passed from one time step to the next. This makes them useful for tasks such as language modeling, machine translation, and speech recognition.\n",
      "\n",
      "YouTube Video:\n",
      "\n",
      "For a more visual explanation of the difference between CNN and RNN, you can watch this video:\n",
      "\n",
      "[CNN vs RNN Explained (with examples) - Machine Learning Basics](https://www.youtube.com/watch?v=J_z6Z-K-Z-Q)\n",
      "\n",
      "This video provides a clear and concise explanation of both CNN and RNN, along with examples of their applications. It is a great resource for anyone looking to understand these important neural network architectures.\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import List, Dict\n",
    "import re, requests\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.utilities.semanticscholar import SemanticScholarAPIWrapper\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled, VideoUnavailable\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Initialize LLM\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Tools\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "API_KEY = \"AIzaSyBNBTgze_5FR5VHzfZlxc38iLwr7xyYaHE\"\n",
    "def youtube_search_guaranteed(query: str, max_results=3) -> List[str]:\n",
    "    search_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "    params = {\"part\":\"snippet\",\"q\":query,\"type\":\"video\",\"maxResults\":20,\"key\":API_KEY}\n",
    "    items = requests.get(search_url, params=params).json().get(\"items\", [])\n",
    "    video_ids = [i['id']['videoId'] for i in items if i.get('id', {}).get('videoId')]\n",
    "    if not video_ids: return []\n",
    "\n",
    "    details_url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "    params = {\"part\":\"status,snippet\",\"id\":\",\".join(video_ids),\"key\":API_KEY}\n",
    "    details_res = requests.get(details_url, params=params).json()\n",
    "    videos = []\n",
    "    for v in details_res.get(\"items\", []):\n",
    "        s = v.get(\"status\", {})\n",
    "        if s.get(\"uploadStatus\") != \"processed\" or s.get(\"privacyStatus\") != \"public\":\n",
    "            continue\n",
    "        snip = v.get(\"snippet\", {})\n",
    "        videos.append(f\"{snip.get('title')} - https://www.youtube.com/watch?v={v.get('id')}\")\n",
    "        if len(videos)>=max_results: break\n",
    "    return videos\n",
    "\n",
    "@tool\n",
    "def youtube_search_agent(query: str) -> str:\n",
    "    \"\"\" Extract main topic and fetch top 3 valid YouTube videos. \"\"\"\n",
    "    cleaned = re.sub(r\"\\b(explain|give me|show|video|tutorial|about|for|the|a|an)\\b\",\"\", query, flags=re.I)\n",
    "    keywords = \" \".join(cleaned.split()[:7])\n",
    "    videos = youtube_search_guaranteed(keywords)\n",
    "    return \"\\n\".join(videos) if videos else \"No videos found.\"\n",
    "\n",
    "@tool\n",
    "def topic_explanation(query: str) -> str:\n",
    "    \"\"\" Return a concise conceptual explanation of the topic. \"\"\"\n",
    "    prompt = f\"Explain '{query}' in 3-5 sentences, beginner-friendly.\"\n",
    "    return hf_model.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "@tool\n",
    "def generate_resume(job_description: str, candidate_info: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a professional LaTeX resume and cover letter.\n",
    "\n",
    "    Args:\n",
    "        job_description (str): Target job description or role.\n",
    "        candidate_info (str, optional): Candidate's info (skills, experience, education).\n",
    "\n",
    "    Returns:\n",
    "        str: LaTeX-formatted resume + cover letter (Overleaf style).\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a professional CV/Resume creator.\n",
    "    Generate a **LaTeX resume** and cover letter using the `moderncv` style\n",
    "    (clean, Overleaf-friendly).\n",
    "\n",
    "    Requirements:\n",
    "    - Must be compile-ready LaTeX code.\n",
    "    - Include:\n",
    "      * Header: Name, Contact (email, phone, LinkedIn, GitHub)\n",
    "      * Summary\n",
    "      * Skills\n",
    "      * Education\n",
    "      * Experience / Projects\n",
    "      * Achievements\n",
    "    - After resume, include a cover letter with proper LaTeX letter formatting.\n",
    "    - Do NOT explain anything, return ONLY valid LaTeX code.\n",
    "\n",
    "    Job Description:\n",
    "    {job_description}\n",
    "\n",
    "    Candidate Info:\n",
    "    {candidate_info if candidate_info else \"General fresher/student profile\"}\n",
    "    \"\"\"\n",
    "    return hf_model.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ss = SemanticScholarAPIWrapper(top_k_results=5, load_max_docs=5)\n",
    "\n",
    "@tool\n",
    "def semantic_scholar_research(query: str, summarize: bool=True) -> List[Dict]:\n",
    "    \"\"\" Fetch top research papers from Semantic Scholar and optionally summarize abstracts. \"\"\"\n",
    "    raw = ss.run(query)\n",
    "    papers = raw.split(\"\\n\\n\")\n",
    "    result=[]\n",
    "    for p in papers:\n",
    "        summary=\"\"\n",
    "        if summarize and \"abstract:\" in p.lower():\n",
    "            summary=hf_model.invoke([HumanMessage(content=f\"Summarize in 2-3 sentences:\\n{p}\")]).content\n",
    "        result.append({\"raw_info\":p,\"summary\":summary})\n",
    "    return result\n",
    "\n",
    "yt_prompt_template = \"\"\"\n",
    "Transcript:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "yt_prompt = PromptTemplate(template=yt_prompt_template, input_variables=[\"context\",\"question\"])\n",
    "\n",
    "def get_transcript(video_id: str):\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi().fetch(video_id, languages=[\"hi\"])\n",
    "    except NoTranscriptFound:\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi().fetch(video_id, languages=[\"en\"])\n",
    "        except NoTranscriptFound: return None, \"No transcript\"\n",
    "    except (TranscriptsDisabled, VideoUnavailable) as e: return None, str(e)\n",
    "    texts = [snippet.get(\"text\",\"\") if isinstance(snippet, dict) else getattr(snippet,\"text\",\"\") for snippet in transcript_list]\n",
    "    return \" \".join(texts), None\n",
    "\n",
    "@tool\n",
    "def youtube_qa(video_url:str, question:str):\n",
    "    \"\"\" Given a YouTube URL and question, return answer based on transcript. \"\"\"\n",
    "    vid_id=video_url.split(\"v=\")[-1].split(\"&\")[0]\n",
    "    transcript,_=get_transcript(vid_id)\n",
    "    rag = ({\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()} | yt_prompt | hf_model)\n",
    "    return rag.invoke({\"context\": transcript, \"question\": question})\n",
    "\n",
    "duck_tool = DuckDuckGoSearchRun()\n",
    "tavily_tool = TavilySearch()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Agents\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "curriculum_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[topic_explanation, youtube_search_agent],\n",
    "    name=\"curriculum_agent\",\n",
    "    prompt=\"You are CURRICULUM-GUIDE. Use topic_explanation + youtube_search_agent.\"\n",
    ")\n",
    "\n",
    "debug_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[youtube_search_agent, duck_tool, tavily_tool],\n",
    "    name=\"debug_agent\",\n",
    "    prompt=\"You are CODE-TUTOR. Debug and review resumes.\"\n",
    ")\n",
    "\n",
    "resume_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[generate_resume],\n",
    "    name=\"resume_agent\",\n",
    "    prompt=\"You are RESUME CREATOR. Generate resumes.\"\n",
    ")\n",
    "\n",
    "news_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[tavily_tool, duck_tool, youtube_search_agent],\n",
    "    name=\"news_agent\",\n",
    "    prompt=\"You are TECH-TREND-GUIDE. Summarize latest tech news.\"\n",
    ")\n",
    "\n",
    "unified_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[youtube_qa, semantic_scholar_research],\n",
    "    name=\"unified_agent\",\n",
    "    prompt=\"Answer YouTube QA & research papers.\"\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Dynamic Intent Router\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def route_query(query: str):\n",
    "    \"\"\"\n",
    "    Dynamically routes user queries to the correct agent using \n",
    "    a weighted scoring system with tie-breaking.\n",
    "    \n",
    "    If query mentions both education and video, video gets priority.\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    # Define keyword sets for each intent\n",
    "    intent_keywords = {\n",
    "        \"curriculum\": [\n",
    "            \"explain\", \"difference between\", \"teach\", \"guide\", \"tutorial\",\n",
    "            \"lesson\", \"steps\", \"how to\", \"define\", \"meaning of\", \"concept\"\n",
    "        ],\n",
    "        \"video\": [\n",
    "            \"summarize this video\", \"video summary\", \"explain video\",\n",
    "            \"youtube summary\", \"summarize yt\", \"video link\", \"show video\"\n",
    "        ],\n",
    "        \"resume_create\": [\n",
    "            \"create resume\", \"make resume\", \"build resume\", \"generate resume\",\n",
    "            \"draft resume\", \"prepare resume\", \"design resume\", \"write resume\",\n",
    "            \"produce resume\", \"craft resume\", \"develop resume\"\n",
    "        ],\n",
    "        \"resume_review\": [\n",
    "            \"review resume\", \"improve resume\", \"fix resume\", \"debug resume\",\n",
    "            \"correct resume\", \"optimize resume\", \"enhance resume\", \"edit resume\",\n",
    "            \"update resume\", \"polish resume\", \"refine resume\", \"analyze resume\"\n",
    "        ],\n",
    "        \"research\": [\n",
    "            \"research paper\", \"summarize research\", \"academic\", \"journal\",\n",
    "            \"study summary\", \"paper review\", \"literature review\"\n",
    "        ],\n",
    "        \"news\": [\n",
    "            \"trend\", \"latest news\", \"breaking\", \"headlines\",\n",
    "            \"today news\", \"current affairs\", \"updates\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Initialize scores\n",
    "    scores = {intent: 0 for intent in intent_keywords.keys()}\n",
    "\n",
    "    # Score calculation\n",
    "    for intent, keywords in intent_keywords.items():\n",
    "        for kw in keywords:\n",
    "            if re.search(rf\"\\b{kw}\\b\", query_lower):\n",
    "                scores[intent] += 1\n",
    "\n",
    "    # Special case: YouTube links ‚Üí strong video signal\n",
    "    if re.search(r\"https?://.*youtube.*\", query_lower):\n",
    "        scores[\"video\"] += 5  # weighted boost\n",
    "\n",
    "    # Pick best matching agent\n",
    "    best_intent = max(scores, key=scores.get)\n",
    "    best_score = scores[best_intent]\n",
    "\n",
    "    # Tie-break: if both curriculum & video are > 0\n",
    "    if scores[\"curriculum\"] > 0 and scores[\"video\"] > 0:\n",
    "        # If user explicitly asks for video ‚Üí route to unified_agent\n",
    "        if any(word in query_lower for word in [\"video\", \"youtube\", \"yt\", \"link\"]):\n",
    "            best_intent = \"video\"\n",
    "        else:\n",
    "            best_intent = \"curriculum\"\n",
    "\n",
    "    # Map intent to agent\n",
    "    if best_score == 0:\n",
    "        return debug_agent  # fallback\n",
    "    elif best_intent == \"curriculum\":\n",
    "        return curriculum_agent\n",
    "    elif best_intent == \"video\":\n",
    "        return unified_agent\n",
    "    elif best_intent == \"resume_create\":\n",
    "        return resume_agent\n",
    "    elif best_intent == \"resume_review\":\n",
    "        return debug_agent\n",
    "    elif best_intent == \"research\":\n",
    "        return unified_agent\n",
    "    elif best_intent == \"news\":\n",
    "        return news_agent\n",
    "    else:\n",
    "        return debug_agent\n",
    "\n",
    "\n",
    "workflow = create_supervisor(\n",
    "    [curriculum_agent, debug_agent, resume_agent, unified_agent, news_agent],\n",
    "    model=hf_model,\n",
    "    prompt=\"SUPERVISOR: Route dynamically based on intent.\"\n",
    ")\n",
    "app = workflow.compile()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Example Run\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    queries=[\n",
    "        \"\"\"what is the difference between supervised and unsupervised learning\"\"\"\n",
    "    ]\n",
    "    # ...existing code...\n",
    "# ...existing code...\n",
    "agent = route_query(q)\n",
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": q}]})\n",
    "for msg in result[\"messages\"]:\n",
    "    if hasattr(msg, \"content\"):\n",
    "        print(f\"{msg.name}:\\n{msg.content}\\n{'='*80}\\n\")\n",
    "# ...existing code...\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e05f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f561d946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c825bc9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ff425484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None:\n",
      "Now based on this job description create a resume ,description=[About the Internship\n",
      "\n",
      "Are you fascinated by data, machine learning, and AI-driven solutions? Do you want to gain hands-on experience in solving real-world business challenges using cutting-edge technology? As a Data Science Intern, you‚Äôll work on projects that directly impact business decisions in the AdTech ecosystem, customer analytics, and generative AI applications.\n",
      "\n",
      "\n",
      "This internship is designed for freshers who are eager to apply their academic knowledge to industry problems. You‚Äôll collaborate with data scientists, engineers, and product managers to design models, analyze data, and deliver insights that power smarter strategies and solutions for clients.\n",
      "\n",
      "\n",
      "Key Responsibilities\n",
      "\n",
      "As part of the team, you‚Äôll contribute to both research and development tasks, ensuring real business impact:\n",
      "\n",
      "Business Translation: Work closely with stakeholders to translate business requirements into solvable data science problems.\n",
      "Algorithm Development: Design and implement algorithms for multi-channel budget and bid optimization, particularly within the Walled Garden AdTech ecosystem.\n",
      "Customer Analytics: Develop solutions for customer segmentation, churn prediction, and other predictive models using large-scale datasets.\n",
      "Machine Learning: Build and train ML models using Python (Pandas, NumPy, Scikit-learn), focusing on real-world applications like ensemble methods, time series modeling, and boosting techniques.\n",
      "Generative AI: Explore and implement solutions with Large Language Models (LLMs) and Generative AI tailored to client needs.\n",
      "Continuous Improvement: Document, evaluate, and enhance algorithms and models for better performance and scalability.\n",
      "Metrics & Outcomes: Define and track key performance indicators (KPIs) to measure the success of proposed solutions.\n",
      "Collaboration: Partner with a cross-functional team of data scientists, engineers, and product managers throughout the product lifecycle.\n",
      "\n",
      "\n",
      "Skills and Qualifications\n",
      "\n",
      "We‚Äôre looking for motivated learners with a solid foundation in data science concepts:\n",
      "\n",
      "Education: Bachelor‚Äôs degree in Computer Science, Statistics, Mathematics, or related field.\n",
      "Programming: Proficiency in Python and experience with libraries such as Pandas, NumPy, and Scikit-learn.\n",
      "SQL: Intermediate-level skills to query, manage, and analyze large datasets.\n",
      "Machine Learning Knowledge: Familiarity with techniques such as regularization, boosting, random forests, ensemble methods, and time series modeling.\n",
      "AI/ML Exposure: Prior experience or academic projects with LLMs and Generative AI.\n",
      "Additional Plus: Knowledge of REST APIs, web services, and production-grade model deployment.\n",
      "Soft Skills: Strong problem-solving mindset, excellent written and verbal communication, and the ability to work effectively in a team.\n",
      "]\n",
      "================================================================================\n",
      "\n",
      "curriculum_agent:\n",
      " **Data Science Intern Resume**\n",
      "\n",
      "**Contact Information**\n",
      "- Name: [Your Name]\n",
      "- Email: [Your Email]\n",
      "- Phone: [Your Phone Number]\n",
      "- LinkedIn: [Your LinkedIn Profile]\n",
      "\n",
      "**Objective**\n",
      "Motivated Computer Science student seeking a Data Science Internship at [Company Name]. Eager to apply academic knowledge to real-world business challenges in the AdTech ecosystem, customer analytics, and generative AI applications. Proficient in Python, SQL, and Machine Learning techniques.\n",
      "\n",
      "**Education**\n",
      "- Bachelor of Science in Computer Science, [University Name], [Graduation Date]\n",
      "\n",
      "**Skills**\n",
      "- Proficient in Python, Pandas, NumPy, Scikit-learn\n",
      "- Intermediate SQL skills\n",
      "- Familiarity with Machine Learning techniques: regularization, boosting, random forests, ensemble methods, time series modeling\n",
      "- Prior experience or academic projects with Large Language Models (LLMs) and Generative AI\n",
      "- Knowledge of REST APIs, web services, and production-grade model deployment (plus)\n",
      "- Strong problem-solving mindset\n",
      "- Excellent written and verbal communication\n",
      "- Ability to work effectively in a team\n",
      "\n",
      "**Projects**\n",
      "- [Project 1 Title]: A data science project where I applied machine learning techniques to solve a real-world problem. [Briefly describe the project, the problem you addressed, the techniques you used, and the results you achieved.]\n",
      "- [Project 2 Title]: An academic project where I explored the use of Large Language Models (LLMs) and Generative AI to solve a specific problem. [Briefly describe the project, the problem you addressed, the techniques you used, and the results you achieved.]\n",
      "\n",
      "**Key Responsibilities**\n",
      "- Translate business requirements into solvable data science problems\n",
      "- Design and implement algorithms for multi-channel budget and bid optimization\n",
      "- Develop solutions for customer segmentation, churn prediction, and other predictive models\n",
      "- Build and train ML models using Python and focus on real-world applications\n",
      "- Document, evaluate, and enhance algorithms and models for better performance and scalability\n",
      "- Define and track key performance indicators (KPIs) to measure the success of proposed solutions\n",
      "- Collaborate with a cross-functional team of data scientists, engineers, and product managers throughout the product lifecycle\n",
      "\n",
      "**Why I'm a Good Fit**\n",
      "I am a highly motivated and detail-oriented individual with a strong foundation in data science concepts. I have a passion for solving real-world business challenges using cutting-edge technology, and I am excited about the opportunity to work on projects that directly impact business decisions in the AdTech ecosystem, customer analytics, and generative AI applications. I am confident that my skills, experience, and enthusiasm make me a strong candidate for the Data Science Intern position at [Company Name].\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import List, Dict\n",
    "import re, requests\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.utilities.semanticscholar import SemanticScholarAPIWrapper\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled, VideoUnavailable\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Initialize LLM\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Tools\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "API_KEY = \"AIzaSyBNBTgze_5FR5VHzfZlxc38iLwr7xyYaHE\"\n",
    "def youtube_search_guaranteed(query: str, max_results=3) -> List[str]:\n",
    "    search_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "    params = {\"part\":\"snippet\",\"q\":query,\"type\":\"video\",\"maxResults\":20,\"key\":API_KEY}\n",
    "    items = requests.get(search_url, params=params).json().get(\"items\", [])\n",
    "    video_ids = [i['id']['videoId'] for i in items if i.get('id', {}).get('videoId')]\n",
    "    if not video_ids: return []\n",
    "\n",
    "    details_url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "    params = {\"part\":\"status,snippet\",\"id\":\",\".join(video_ids),\"key\":API_KEY}\n",
    "    details_res = requests.get(details_url, params=params).json()\n",
    "    videos = []\n",
    "    for v in details_res.get(\"items\", []):\n",
    "        s = v.get(\"status\", {})\n",
    "        if s.get(\"uploadStatus\") != \"processed\" or s.get(\"privacyStatus\") != \"public\":\n",
    "            continue\n",
    "        snip = v.get(\"snippet\", {})\n",
    "        videos.append(f\"{snip.get('title')} - https://www.youtube.com/watch?v={v.get('id')}\")\n",
    "        if len(videos)>=max_results: break\n",
    "    return videos\n",
    "\n",
    "@tool\n",
    "def youtube_search_agent(query: str) -> str:\n",
    "    \"\"\" Extract main topic and fetch top 3 valid YouTube videos. \"\"\"\n",
    "    cleaned = re.sub(r\"\\b(explain|give me|show|video|tutorial|about|for|the|a|an)\\b\",\"\", query, flags=re.I)\n",
    "    keywords = \" \".join(cleaned.split()[:7])\n",
    "    videos = youtube_search_guaranteed(keywords)\n",
    "    return \"\\n\".join(videos) if videos else \"No videos found.\"\n",
    "\n",
    "@tool\n",
    "def topic_explanation(query: str) -> str:\n",
    "    \"\"\" Return a concise conceptual explanation of the topic. \"\"\"\n",
    "    prompt = f\"Explain '{query}' in 3-5 sentences, beginner-friendly.\"\n",
    "    return hf_model.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "@tool\n",
    "def generate_resume(job_description: str, candidate_info: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a professional LaTeX resume and cover letter.\n",
    "\n",
    "    Args:\n",
    "        job_description (str): Target job description or role.\n",
    "        candidate_info (str, optional): Candidate's info (skills, experience, education).\n",
    "\n",
    "    Returns:\n",
    "        str: LaTeX-formatted resume + cover letter (Overleaf style).\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a professional CV/Resume creator.\n",
    "    Generate a **LaTeX resume** and cover letter using the `moderncv` style\n",
    "    (clean, Overleaf-friendly).\n",
    "\n",
    "    Requirements:\n",
    "    - Must be compile-ready LaTeX code.\n",
    "    - Include:\n",
    "      * Header: Name, Contact (email, phone, LinkedIn, GitHub)\n",
    "      * Summary\n",
    "      * Skills\n",
    "      * Education\n",
    "      * Experience / Projects\n",
    "      * Achievements\n",
    "    - After resume, include a cover letter with proper LaTeX letter formatting.\n",
    "    - Do NOT explain anything, return ONLY valid LaTeX code.\n",
    "\n",
    "    Job Description:\n",
    "    {job_description}\n",
    "\n",
    "    Candidate Info:\n",
    "    {candidate_info if candidate_info else \"General fresher/student profile\"}\n",
    "    \"\"\"\n",
    "    return hf_model.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ss = SemanticScholarAPIWrapper(top_k_results=5, load_max_docs=5)\n",
    "\n",
    "@tool\n",
    "def semantic_scholar_research(query: str, summarize: bool=True) -> List[Dict]:\n",
    "    \"\"\" Fetch top research papers from Semantic Scholar and optionally summarize abstracts. \"\"\"\n",
    "    raw = ss.run(query)\n",
    "    papers = raw.split(\"\\n\\n\")\n",
    "    result=[]\n",
    "    for p in papers:\n",
    "        summary=\"\"\n",
    "        if summarize and \"abstract:\" in p.lower():\n",
    "            summary=hf_model.invoke([HumanMessage(content=f\"Summarize in 2-3 sentences:\\n{p}\")]).content\n",
    "        result.append({\"raw_info\":p,\"summary\":summary})\n",
    "    return result\n",
    "\n",
    "yt_prompt_template = \"\"\"\n",
    "Transcript:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "yt_prompt = PromptTemplate(template=yt_prompt_template, input_variables=[\"context\",\"question\"])\n",
    "\n",
    "def get_transcript(video_id: str):\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi().fetch(video_id, languages=[\"hi\"])\n",
    "    except NoTranscriptFound:\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi().fetch(video_id, languages=[\"en\"])\n",
    "        except NoTranscriptFound: return None, \"No transcript\"\n",
    "    except (TranscriptsDisabled, VideoUnavailable) as e: return None, str(e)\n",
    "    texts = [snippet.get(\"text\",\"\") if isinstance(snippet, dict) else getattr(snippet,\"text\",\"\") for snippet in transcript_list]\n",
    "    return \" \".join(texts), None\n",
    "\n",
    "@tool\n",
    "def youtube_qa(video_url:str, question:str):\n",
    "    \"\"\" Given a YouTube URL and question, return answer based on transcript. \"\"\"\n",
    "    vid_id=video_url.split(\"v=\")[-1].split(\"&\")[0]\n",
    "    transcript,_=get_transcript(vid_id)\n",
    "    rag = ({\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()} | yt_prompt | hf_model)\n",
    "    return rag.invoke({\"context\": transcript, \"question\": question})\n",
    "\n",
    "duck_tool = DuckDuckGoSearchRun()\n",
    "tavily_tool = TavilySearch()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Agents\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "curriculum_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[topic_explanation, youtube_search_agent],\n",
    "    name=\"curriculum_agent\",\n",
    "    prompt=\"You are CURRICULUM-GUIDE. Use topic_explanation + youtube_search_agent.\"\n",
    ")\n",
    "\n",
    "debug_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[youtube_search_agent, duck_tool, tavily_tool],\n",
    "    name=\"debug_agent\",\n",
    "    prompt=\"You are CODE-TUTOR. Debug and review resumes.\"\n",
    ")\n",
    "\n",
    "resume_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[generate_resume],\n",
    "    name=\"resume_agent\",\n",
    "    prompt=\"You are RESUME CREATOR. Generate resumes.\"\n",
    ")\n",
    "\n",
    "news_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[tavily_tool, duck_tool, youtube_search_agent],\n",
    "    name=\"news_agent\",\n",
    "    prompt=\"You are TECH-TREND-GUIDE. Summarize latest tech news.\"\n",
    ")\n",
    "\n",
    "unified_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[youtube_qa, semantic_scholar_research],\n",
    "    name=\"unified_agent\",\n",
    "    prompt=\"Answer YouTube QA & research papers.\"\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Dynamic Intent Router\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import re\n",
    "\n",
    "def route_query(query: str):\n",
    "    \"\"\"\n",
    "    Dynamically routes user queries to the correct agent using \n",
    "    a weighted scoring system with tie-breaking.\n",
    "    \n",
    "    If query mentions both education and video, video gets priority.\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    # Define keyword sets for each intent\n",
    "    intent_keywords = {\n",
    "        \"curriculum\": [\n",
    "            \"explain\", \"difference between\", \"teach\", \"guide\", \"tutorial\",\n",
    "            \"lesson\", \"steps\", \"how to\", \"define\", \"meaning of\", \"concept\"\n",
    "        ],\n",
    "        \"video\": [\n",
    "            \"summarize this video\", \"video summary\", \"explain video\",\n",
    "            \"youtube summary\", \"summarize yt\", \"video link\", \"show video\"\n",
    "        ],\n",
    "        \"resume_create\": [\n",
    "            \"create resume\", \"make resume\", \"build resume\", \"generate resume\",\n",
    "            \"draft resume\", \"prepare resume\", \"design resume\", \"write resume\",\n",
    "            \"produce resume\", \"craft resume\", \"develop resume\"\n",
    "        ],\n",
    "        \"resume_review\": [\n",
    "            \"review resume\", \"improve resume\", \"fix resume\", \"debug resume\",\n",
    "            \"correct resume\", \"optimize resume\", \"enhance resume\", \"edit resume\",\n",
    "            \"update resume\", \"polish resume\", \"refine resume\", \"analyze resume\"\n",
    "        ],\n",
    "        \"research\": [\n",
    "            \"research paper\", \"summarize research\", \"academic\", \"journal\",\n",
    "            \"study summary\", \"paper review\", \"literature review\"\n",
    "        ],\n",
    "        \"news\": [\n",
    "            \"trend\", \"latest news\", \"breaking\", \"headlines\",\n",
    "            \"today news\", \"current affairs\", \"updates\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Initialize scores\n",
    "    scores = {intent: 0 for intent in intent_keywords.keys()}\n",
    "\n",
    "    # Score calculation\n",
    "    for intent, keywords in intent_keywords.items():\n",
    "        for kw in keywords:\n",
    "            if re.search(rf\"\\b{kw}\\b\", query_lower):\n",
    "                scores[intent] += 1\n",
    "\n",
    "    # Special case: YouTube links ‚Üí strong video signal\n",
    "    if re.search(r\"https?://.*youtube.*\", query_lower):\n",
    "        scores[\"video\"] += 5  # weighted boost\n",
    "\n",
    "    # Pick best matching agent\n",
    "    best_intent = max(scores, key=scores.get)\n",
    "    best_score = scores[best_intent]\n",
    "\n",
    "    # Tie-break: if both curriculum & video are > 0\n",
    "    if scores[\"curriculum\"] > 0 and scores[\"video\"] > 0:\n",
    "        # If user explicitly asks for video ‚Üí route to unified_agent\n",
    "        if any(word in query_lower for word in [\"video\", \"youtube\", \"yt\", \"link\"]):\n",
    "            best_intent = \"video\"\n",
    "        else:\n",
    "            best_intent = \"curriculum\"\n",
    "\n",
    "    # Map intent to agent\n",
    "    if best_score == 0:\n",
    "        return debug_agent  # fallback\n",
    "    elif best_intent == \"curriculum\":\n",
    "        return curriculum_agent\n",
    "    elif best_intent == \"video\":\n",
    "        return unified_agent\n",
    "    elif best_intent == \"resume_create\":\n",
    "        return resume_agent\n",
    "    elif best_intent == \"resume_review\":\n",
    "        return debug_agent\n",
    "    elif best_intent == \"research\":\n",
    "        return unified_agent\n",
    "    elif best_intent == \"news\":\n",
    "        return news_agent\n",
    "    else:\n",
    "        return debug_agent\n",
    "\n",
    "\n",
    "workflow = create_supervisor(\n",
    "    [curriculum_agent, debug_agent, resume_agent, unified_agent, news_agent],\n",
    "    model=hf_model,\n",
    "    prompt=\"SUPERVISOR: Route dynamically based on intent.\"\n",
    ")\n",
    "app = workflow.compile()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Example Run\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if __name__==\"__main__\":\n",
    "    queries=[\n",
    "        \"\"\"Now based on this job description create a resume ,description=[About the Internship\n",
    "\n",
    "Are you fascinated by data, machine learning, and AI-driven solutions? Do you want to gain hands-on experience in solving real-world business challenges using cutting-edge technology? As a Data Science Intern, you‚Äôll work on projects that directly impact business decisions in the AdTech ecosystem, customer analytics, and generative AI applications.\n",
    "\n",
    "\n",
    "This internship is designed for freshers who are eager to apply their academic knowledge to industry problems. You‚Äôll collaborate with data scientists, engineers, and product managers to design models, analyze data, and deliver insights that power smarter strategies and solutions for clients.\n",
    "\n",
    "\n",
    "Key Responsibilities\n",
    "\n",
    "As part of the team, you‚Äôll contribute to both research and development tasks, ensuring real business impact:\n",
    "\n",
    "Business Translation: Work closely with stakeholders to translate business requirements into solvable data science problems.\n",
    "Algorithm Development: Design and implement algorithms for multi-channel budget and bid optimization, particularly within the Walled Garden AdTech ecosystem.\n",
    "Customer Analytics: Develop solutions for customer segmentation, churn prediction, and other predictive models using large-scale datasets.\n",
    "Machine Learning: Build and train ML models using Python (Pandas, NumPy, Scikit-learn), focusing on real-world applications like ensemble methods, time series modeling, and boosting techniques.\n",
    "Generative AI: Explore and implement solutions with Large Language Models (LLMs) and Generative AI tailored to client needs.\n",
    "Continuous Improvement: Document, evaluate, and enhance algorithms and models for better performance and scalability.\n",
    "Metrics & Outcomes: Define and track key performance indicators (KPIs) to measure the success of proposed solutions.\n",
    "Collaboration: Partner with a cross-functional team of data scientists, engineers, and product managers throughout the product lifecycle.\n",
    "\n",
    "\n",
    "Skills and Qualifications\n",
    "\n",
    "We‚Äôre looking for motivated learners with a solid foundation in data science concepts:\n",
    "\n",
    "Education: Bachelor‚Äôs degree in Computer Science, Statistics, Mathematics, or related field.\n",
    "Programming: Proficiency in Python and experience with libraries such as Pandas, NumPy, and Scikit-learn.\n",
    "SQL: Intermediate-level skills to query, manage, and analyze large datasets.\n",
    "Machine Learning Knowledge: Familiarity with techniques such as regularization, boosting, random forests, ensemble methods, and time series modeling.\n",
    "AI/ML Exposure: Prior experience or academic projects with LLMs and Generative AI.\n",
    "Additional Plus: Knowledge of REST APIs, web services, and production-grade model deployment.\n",
    "Soft Skills: Strong problem-solving mindset, excellent written and verbal communication, and the ability to work effectively in a team.\n",
    "]\"\"\"\n",
    "   \n",
    "        \n",
    "    ]\n",
    "    for q in queries:\n",
    "        agent = route_query(q)\n",
    "        result = agent.invoke({\"messages\":[{\"role\":\"user\",\"content\":q}]})\n",
    "        for msg in result[\"messages\"]:\n",
    "            if hasattr(msg,'content'):\n",
    "                print(f\"{msg.name}:\\n{msg.content}\\n{'='*80}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae3073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "c2a2a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[youtube_qa, semantic_scholar_research],\n",
    "    name=\"unified_agent\",  # <-- rename from \"knowledge_agent\"\n",
    "    prompt=\"\"\"\n",
    "You are UNIFIED-AGENT, capable of handling YouTube transcript QA and research paper summarization.\n",
    "- If a valid YouTube link is provided, use youtube_qa to answer or summarize the transcript.\n",
    "- If a research query or PDF link is provided, use semantic_scholar_research to fetch and summarize papers.\n",
    "- Indicate the source clearly (video or research paper).\n",
    "- If info is unavailable, say so explicitly.\n",
    "- Always provide concise, clear, and user-friendly answers.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "58087bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curriculum_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[duck_tool, youtube_search],\n",
    "    name=\"curriculum_agent\",\n",
    "    prompt=\"\"\"\n",
    "You are CURRICULUM-GUIDE, a knowledgeable educational expert.\n",
    "\n",
    "Rules:\n",
    "\n",
    "1. Generate a **personalized learning roadmap** based on the user's query.\n",
    "2. Always fetch **top 3 YouTube videos** for the topic using the `youtube_search` tool.\n",
    "   - Include **full video title and URL**.\n",
    "   - Do not truncate, summarize, or remove links.\n",
    "3. If no official video is found, output: \"No related videos found.\"\n",
    "4. Include step-by-step guidance in the roadmap.\n",
    "5. Return only the **final clean output**, do not include internal agent thoughts, actions, or debug messages.\n",
    "6. Format the response like this:\n",
    "\n",
    "- **Course/Topic Name:** [Name]\n",
    "- **Description:** [Brief summary]\n",
    "- **YouTube Videos:** \n",
    "    [Title 1] - [URL 1]\n",
    "    [Title 2] - [URL 2]\n",
    "    [Title 3] - [URL 3]\n",
    "- **Learning Steps:** \n",
    "    1. Step one\n",
    "    2. Step two\n",
    "    3. Step three\n",
    "\n",
    "Always use the **exact output from the youtube_search tool** under \"YouTube Videos\".\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "16e59ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[youtube_search, duck_tool, tavile_tool],\n",
    "    name=\"debug_educational_agent\",\n",
    "    prompt=\"\"\"\n",
    "You are CODE-TUTOR, a friendly and knowledgeable AI teacher and coding mentor. \n",
    "When a student asks a question or submits code, follow these rules:\n",
    "\n",
    "1. **Explain concepts clearly** in an educational and beginner-friendly manner.\n",
    "2. **Debug the code**: identify errors, explain why they occur, and suggest precise fixes.\n",
    "3. **Provide learning resources**: include relevant YouTube links or tutorials . \n",
    "4. **Give examples, exercises, or mini-quizzes** when helpful to reinforce understanding.\n",
    "5. **Motivate the student**: encourage practice, exploration, and curiosity.\n",
    "6. **Step-by-step guidance**: never give one-line answers; always walk the student through solutions.\n",
    "7. **Format output clearly**: use numbered steps, bullet points, or code blocks where applicable.\n",
    "\n",
    "Goal: Deliver comprehensive, interactive, and motivational educational guidance with youtube video.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c1db9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[generate_resume],\n",
    "    name=\"resume_agent\",\n",
    "    prompt=\"\"\"\n",
    "You are RESUME-GUIDE, an expert in generating resumes and cover letters. Your task is to produce\n",
    "a high-quality, professional, structured resume and cover letter for the given job description.\n",
    "\n",
    "Rules:\n",
    "1. Use the provided generate_resume tool for producing HTML output.\n",
    "2. Tailor content dynamically to match the job description.\n",
    "3. Ensure output is balanced: concise yet informative, highlighting key skills, achievements, and fit.\n",
    "4. Internally iterate if needed to refine clarity, professionalism, and formatting.\n",
    "5. Return only the **final polished HTML output** without any internal messages.\n",
    "\n",
    "Sections to include:\n",
    "- Summary\n",
    "- Skills\n",
    "- Experience\n",
    "- Cover Letter\n",
    "\n",
    "Goal: Generate a professional, polished, and ready-to-use resume + cover letter.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2ab95158",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[tavile_tool, duck_tool, youtube_search],\n",
    "    name=\"news_agent\",\n",
    "    prompt=\"\"\"\n",
    "You are TECH-TREND-GUIDE, a proactive and knowledgeable technology analyst. Your task:\n",
    "\n",
    "1. Fetch the latest and trending technology news from multiple sources dynamically.\n",
    "2. Summarize each news point clearly in **point-wise format** for quick reading.\n",
    "3. Highlight **emerging tools, frameworks, and technologies** mentioned or relevant.\n",
    "4. Include **YouTube videos** if they are educational, relevant, and verified. If no video is available, skip gracefully.\n",
    "5. Provide **actionable insights or recommendations** wherever possible.\n",
    "6. Decide intelligently which tool (tavile_tool, duck_tool, or youtube_search) to use for each query.\n",
    "7. Keep the summary **concise, engaging, and readable**; avoid overly long paragraphs.\n",
    "8. Return **only the final summarized news**, do not include internal reasoning or tool calls.\n",
    "\n",
    "Example output format:\n",
    "- News Point 1: [Brief summary]  \n",
    "  Video (if available): [YouTube link]  \n",
    "- News Point 2: [Brief summary]  \n",
    "  Video (if available): [YouTube link]  \n",
    "- Trending Tools/Tech: [List of tools or technologies]  \n",
    "- Recommendation: [Optional actionable insight]\n",
    "\n",
    "Focus on delivering a **dynamic, insightful, and agentic summary of tech trends**, with links only when they add real value.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e707b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = create_supervisor(\n",
    "    [\n",
    "        curriculum_agent,\n",
    "        debug_agent,\n",
    "        resume_agent,\n",
    "        unified_agent,  # Handles YouTube QA & research papers\n",
    "        news_agent\n",
    "    ],\n",
    "    model=hf_model,\n",
    "    prompt=\"\"\"\n",
    "You are SUPERVISOR-GUIDE. Route user queries to the correct agent and return a concise, clear, final response.\n",
    "Always provide structured, user-friendly output with valid links where applicable.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "4d9e9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ef224554",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_query = (\n",
    "        \"\"\"Now based on this job description create a resume ,description=[About the Internship\n",
    "\n",
    "Are you fascinated by data, machine learning, and AI-driven solutions? Do you want to gain hands-on experience in solving real-world business challenges using cutting-edge technology? As a Data Science Intern, you‚Äôll work on projects that directly impact business decisions in the AdTech ecosystem, customer analytics, and generative AI applications.\n",
    "\n",
    "\n",
    "This internship is designed for freshers who are eager to apply their academic knowledge to industry problems. You‚Äôll collaborate with data scientists, engineers, and product managers to design models, analyze data, and deliver insights that power smarter strategies and solutions for clients.\n",
    "\n",
    "\n",
    "Key Responsibilities\n",
    "\n",
    "As part of the team, you‚Äôll contribute to both research and development tasks, ensuring real business impact:\n",
    "\n",
    "Business Translation: Work closely with stakeholders to translate business requirements into solvable data science problems.\n",
    "Algorithm Development: Design and implement algorithms for multi-channel budget and bid optimization, particularly within the Walled Garden AdTech ecosystem.\n",
    "Customer Analytics: Develop solutions for customer segmentation, churn prediction, and other predictive models using large-scale datasets.\n",
    "Machine Learning: Build and train ML models using Python (Pandas, NumPy, Scikit-learn), focusing on real-world applications like ensemble methods, time series modeling, and boosting techniques.\n",
    "Generative AI: Explore and implement solutions with Large Language Models (LLMs) and Generative AI tailored to client needs.\n",
    "Continuous Improvement: Document, evaluate, and enhance algorithms and models for better performance and scalability.\n",
    "Metrics & Outcomes: Define and track key performance indicators (KPIs) to measure the success of proposed solutions.\n",
    "Collaboration: Partner with a cross-functional team of data scientists, engineers, and product managers throughout the product lifecycle.\n",
    "\n",
    "\n",
    "Skills and Qualifications\n",
    "\n",
    "We‚Äôre looking for motivated learners with a solid foundation in data science concepts:\n",
    "\n",
    "Education: Bachelor‚Äôs degree in Computer Science, Statistics, Mathematics, or related field.\n",
    "Programming: Proficiency in Python and experience with libraries such as Pandas, NumPy, and Scikit-learn.\n",
    "SQL: Intermediate-level skills to query, manage, and analyze large datasets.\n",
    "Machine Learning Knowledge: Familiarity with techniques such as regularization, boosting, random forests, ensemble methods, and time series modeling.\n",
    "AI/ML Exposure: Prior experience or academic projects with LLMs and Generative AI.\n",
    "Additional Plus: Knowledge of REST APIs, web services, and production-grade model deployment.\n",
    "Soft Skills: Strong problem-solving mindset, excellent written and verbal communication, and the ability to work effectively in a team.\n",
    "]\"\"\"\n",
    "    )\n",
    "    \n",
    "result = app.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": student_query}]\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "88432ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Now based on this job description create a resume ,description=[About the Internship\n",
      "\n",
      "Are you fascinated by data, machine learning, and AI-driven solutions? Do you want to gain hands-on experience in solving real-world business challenges using cutting-edge technology? As a Data Science Intern, you‚Äôll work on projects that directly impact business decisions in the AdTech ecosystem, customer analytics, and generative AI applications.\n",
      "\n",
      "\n",
      "This internship is designed for freshers who are eager to apply their academic knowledge to industry problems. You‚Äôll collaborate with data scientists, engineers, and product managers to design models, analyze data, and deliver insights that power smarter strategies and solutions for clients.\n",
      "\n",
      "\n",
      "Key Responsibilities\n",
      "\n",
      "As part of the team, you‚Äôll contribute to both research and development tasks, ensuring real business impact:\n",
      "\n",
      "Business Translation: Work closely with stakeholders to translate business requirements into solvable data science problems.\n",
      "Algorithm Development: Design and implement algorithms for multi-channel budget and bid optimization, particularly within the Walled Garden AdTech ecosystem.\n",
      "Customer Analytics: Develop solutions for customer segmentation, churn prediction, and other predictive models using large-scale datasets.\n",
      "Machine Learning: Build and train ML models using Python (Pandas, NumPy, Scikit-learn), focusing on real-world applications like ensemble methods, time series modeling, and boosting techniques.\n",
      "Generative AI: Explore and implement solutions with Large Language Models (LLMs) and Generative AI tailored to client needs.\n",
      "Continuous Improvement: Document, evaluate, and enhance algorithms and models for better performance and scalability.\n",
      "Metrics & Outcomes: Define and track key performance indicators (KPIs) to measure the success of proposed solutions.\n",
      "Collaboration: Partner with a cross-functional team of data scientists, engineers, and product managers throughout the product lifecycle.\n",
      "\n",
      "\n",
      "Skills and Qualifications\n",
      "\n",
      "We‚Äôre looking for motivated learners with a solid foundation in data science concepts:\n",
      "\n",
      "Education: Bachelor‚Äôs degree in Computer Science, Statistics, Mathematics, or related field.\n",
      "Programming: Proficiency in Python and experience with libraries such as Pandas, NumPy, and Scikit-learn.\n",
      "SQL: Intermediate-level skills to query, manage, and analyze large datasets.\n",
      "Machine Learning Knowledge: Familiarity with techniques such as regularization, boosting, random forests, ensemble methods, and time series modeling.\n",
      "AI/ML Exposure: Prior experience or academic projects with LLMs and Generative AI.\n",
      "Additional Plus: Knowledge of REST APIs, web services, and production-grade model deployment.\n",
      "Soft Skills: Strong problem-solving mindset, excellent written and verbal communication, and the ability to work effectively in a team.\n",
      "]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: supervisor\n",
      "Tool Calls:\n",
      "  transfer_to_resume_agent (call_zbl5tp2scssqhhp05lz3d240)\n",
      " Call ID: call_zbl5tp2scssqhhp05lz3d240\n",
      "  Args:\n",
      "    description: Are you fascinated by data, machine learning, and AI-driven solutions? Do you want to gain hands-on experience in solving real-world business challenges using cutting-edge technology? As a Data Science Intern, you‚Äôll work on projects that directly impact business decisions in the AdTech ecosystem, customer analytics, and generative AI applications.\n",
      "\n",
      "This internship is designed for freshers who are eager to apply their academic knowledge to industry problems. You\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_to_resume_agent\n",
      "\n",
      "Successfully transferred to resume_agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: resume_agent\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Data Science Intern Resume</title>\n",
      "    <style>\n",
      "        body {\n",
      "            font-family: Arial, sans-serif;\n",
      "            margin: 0;\n",
      "            padding: 0;\n",
      "        }\n",
      "        .section {\n",
      "            margin-bottom: 20px;\n",
      "        }\n",
      "        .section h2 {\n",
      "            margin-top: 0;\n",
      "        }\n",
      "        .section ul {\n",
      "            list-style: none;\n",
      "            padding: 0;\n",
      "            margin: 0;\n",
      "        }\n",
      "        .section li {\n",
      "            margin-bottom: 10px;\n",
      "        }\n",
      "        .section ul li {\n",
      "            margin-bottom: 10px;\n",
      "        }\n",
      "        .section p {\n",
      "            margin-bottom: 20px;\n",
      "        }\n",
      "    </style>\n",
      "</head>\n",
      "<body>\n",
      "    <header>\n",
      "        <h1>Data Science Intern</h1>\n",
      "    </header>\n",
      "    <section class=\"summary\">\n",
      "        <h2>Summary</h2>\n",
      "        <p>Fresh graduate with a strong foundation in data science concepts, seeking a Data Science Internship to apply academic knowledge to industry problems and gain hands-on experience in solving real-world business challenges using cutting-edge technology.</p>\n",
      "    </section>\n",
      "    <section class=\"skills\">\n",
      "        <h2>Skills</h2>\n",
      "        <ul>\n",
      "            <li>Programming: Proficient in Python and experience with libraries such as Pandas, NumPy, and Scikit-learn.</li>\n",
      "            <li>SQL: Intermediate-level skills to query, manage, and analyze large datasets.</li>\n",
      "            <li>Machine Learning Knowledge: Familiarity with techniques such as regularization, boosting, random forests, ensemble methods, and time series modeling.</li>\n",
      "            <li>AI/ML Exposure: Prior experience or academic projects with LLMs and Generative AI.</li>\n",
      "            <li>Additional Plus: Knowledge of REST APIs, web services, and production-grade model deployment.</li>\n",
      "        </ul>\n",
      "    </section>\n",
      "    <section class=\"experience\">\n",
      "        <h2>Experience</h2>\n",
      "        <ul>\n",
      "            <li>\n",
      "                <h3>Academic Projects</h3>\n",
      "                <p>Designed and implemented algorithms for multi-channel budget and bid optimization, customer segmentation, churn prediction, and other predictive models using large-scale datasets.</p>\n",
      "            </li>\n",
      "            <li>\n",
      "                <h3>Personal Projects</h3>\n",
      "                <p>Developed solutions for customer analytics, generative AI, and machine learning using Python and libraries such as Pandas, NumPy, and Scikit-learn.</p>\n",
      "            </li>\n",
      "        </ul>\n",
      "    </section>\n",
      "    <section class=\"cover-letter\">\n",
      "        <h2>Cover Letter</h2>\n",
      "        <p>\n",
      "            Dear Hiring Manager,\n",
      "            I am excited to apply for the Data Science Intern position at [Company Name]. As a fresh graduate with a strong foundation in data science concepts, I am eager to apply my academic knowledge to industry problems and gain hands-on experience in solving real-world business challenges using cutting-edge technology.\n",
      "            I am confident that my skills and experience make me an ideal candidate for this position. I am proficient in Python and have experience with libraries such as Pandas, NumPy, and Scikit-learn. I also have intermediate-level skills in SQL and familiarity with machine learning techniques such as regularization, boosting, random forests, ensemble methods, and time series modeling.\n",
      "            I am particularly drawn to this internship because of the opportunity to work on projects that directly impact business decisions in the AdTech ecosystem, customer analytics, and generative AI applications. I am excited about the prospect of collaborating with a cross-functional team of data scientists, engineers, and product managers to design models, analyze data, and deliver insights that power smarter strategies and solutions for clients.\n",
      "            Thank you for considering my application. I look forward to the opportunity to discuss my qualifications further.\n",
      "            Sincerely,\n",
      "            [Your Name]\n",
      "        </p>\n",
      "    </section>\n",
      "</body>\n",
      "</html>\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: resume_agent\n",
      "\n",
      "Transferring back to supervisor\n",
      "Tool Calls:\n",
      "  transfer_back_to_supervisor (444cd5f7-8571-4c9d-b3d8-91acc4ef0ca7)\n",
      " Call ID: 444cd5f7-8571-4c9d-b3d8-91acc4ef0ca7\n",
      "  Args:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_back_to_supervisor\n",
      "\n",
      "Successfully transferred back to supervisor\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: supervisor\n",
      "\n",
      "No transcript available; refer to video description or other sources.\n"
     ]
    }
   ],
   "source": [
    "for msg in result[\"messages\"]:\n",
    "    msg.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d8aa7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    student_query = (\n",
    "        \"\"\"Explain the difference between supervised and unsupervised machine learning with yutube video\"\"\"\n",
    "    )\n",
    "    \n",
    "    result = app.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": student_query}]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "5ccc6cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None:\n",
      "Explain the difference between supervised and unsupervised machine learning with yutube video\n",
      "\n",
      "================================================================================\n",
      "\n",
      "supervisor:\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "transfer_to_debug_educational_agent:\n",
      "Successfully transferred to debug_educational_agent\n",
      "\n",
      "================================================================================\n",
      "\n",
      "debug_educational_agent:\n",
      "**Supervised vs Unsupervised Machine Learning: A Clear Explanation**\n",
      "=================================================================\n",
      "\n",
      "In machine learning, we have two primary types of learning: supervised and unsupervised. Understanding the difference between these two is crucial for building accurate and effective models.\n",
      "\n",
      "**Supervised Learning**\n",
      "--------------------\n",
      "\n",
      "Supervised learning is a type of machine learning where the model is trained on labeled data. This means that the data is already annotated with the correct output, and the model learns to map inputs to outputs based on these labels.\n",
      "\n",
      "**Example:** Image classification, where the model is trained on images of dogs and cats, and the labels indicate whether the image is a dog or a cat.\n",
      "\n",
      "**How it works:**\n",
      "\n",
      "1. The model is trained on a dataset of labeled examples.\n",
      "2. The model learns to recognize patterns in the data and make predictions based on these patterns.\n",
      "3. The model is evaluated on a test dataset to measure its performance.\n",
      "\n",
      "**Unsupervised Learning**\n",
      "----------------------\n",
      "\n",
      "Unsupervised learning is a type of machine learning where the model is trained on unlabeled data. This means that the data does not have any inherent structure or meaning, and the model must discover patterns and relationships on its own.\n",
      "\n",
      "**Example:** Clustering customers based on their buying behavior, where the model groups similar customers together without any prior knowledge of their behavior.\n",
      "\n",
      "**How it works:**\n",
      "\n",
      "1. The model is trained on a dataset of unlabeled examples.\n",
      "2. The model discovers patterns and relationships in the data through clustering, dimensionality reduction, or other techniques.\n",
      "3. The model is evaluated on a test dataset to measure its performance.\n",
      "\n",
      "**Key differences:**\n",
      "\n",
      "* **Labeled vs Unlabeled Data:** Supervised learning uses labeled data, while unsupervised learning uses unlabeled data.\n",
      "* **Model Objective:** Supervised learning aims to minimize the error between predicted and actual outputs, while unsupervised learning aims to discover patterns and relationships in the data.\n",
      "* **Evaluation Metrics:** Supervised learning uses metrics such as accuracy, precision, and recall, while unsupervised learning uses metrics such as silhouette score, calinski-harabasz index, and davies-bouldin index.\n",
      "\n",
      "**Code Example:**\n",
      "```python\n",
      "# Supervised Learning Example (Image Classification)\n",
      "from tensorflow.keras.datasets import mnist\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.layers import Dense, Dropout\n",
      "\n",
      "# Load MNIST dataset\n",
      "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
      "\n",
      "# Reshape and normalize data\n",
      "x_train = x_train.reshape(-1, 784)\n",
      "x_test = x_test.reshape(-1, 784)\n",
      "x_train = x_train.astype('float32') / 255\n",
      "x_test = x_test.astype('float32') / 255\n",
      "\n",
      "# Define model architecture\n",
      "model = Sequential()\n",
      "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Dense(10, activation='softmax'))\n",
      "\n",
      "# Compile model\n",
      "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "# Train model\n",
      "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n",
      "\n",
      "# Evaluate model\n",
      "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
      "print(f'Test accuracy: {test_acc:.2f}')\n",
      "```\n",
      "\n",
      "```python\n",
      "# Unsupervised Learning Example (Clustering Customers)\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.datasets import make_blobs\n",
      "\n",
      "# Generate sample data\n",
      "X, _ = make_blobs(n_samples=100, centers=5, n_features=2, random_state=0)\n",
      "\n",
      "# Define clustering model\n",
      "kmeans = KMeans(n_clusters=5, random_state=0)\n",
      "\n",
      "# Fit model\n",
      "kmeans.fit(X)\n",
      "\n",
      "# Get cluster labels\n",
      "labels = kmeans.labels_\n",
      "\n",
      "# Print cluster labels\n",
      "print(labels)\n",
      "```\n",
      "\n",
      "**Practice Exercise:**\n",
      "\n",
      "1. Train a supervised learning model on the MNIST dataset to classify handwritten digits.\n",
      "2. Train an unsupervised learning model on the Iris dataset to cluster flowers based on their characteristics.\n",
      "\n",
      "**Learning Resources:**\n",
      "\n",
      "* \"Supervised Learning\" by Andrew Ng (Coursera)\n",
      "* \"Unsupervised Learning\" by Andrew Ng (Coursera)\n",
      "* \"Machine Learning\" by Scikit-learn (Python library)\n",
      "* \"K-Means Clustering\" by Scikit-learn (Python library)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "debug_educational_agent:\n",
      "Transferring back to supervisor\n",
      "\n",
      "================================================================================\n",
      "\n",
      "transfer_back_to_supervisor:\n",
      "Successfully transferred back to supervisor\n",
      "\n",
      "================================================================================\n",
      "\n",
      "supervisor:\n",
      "I can provide more information on supervised and unsupervised machine learning if you'd like.\n",
      "\n",
      "**Supervised Learning:**\n",
      "\n",
      "Supervised learning is a type of machine learning where the model is trained on labeled data. This means that the data is already annotated with the correct output, and the model learns to map inputs to outputs based on these labels.\n",
      "\n",
      "**Types of Supervised Learning:**\n",
      "\n",
      "1. **Classification:** The model predicts a categorical label or class.\n",
      "2. **Regression:** The model predicts a continuous output value.\n",
      "\n",
      "**Common Supervised Learning Algorithms:**\n",
      "\n",
      "1. **Linear Regression:** A linear model that predicts a continuous output value.\n",
      "2. **Logistic Regression:** A linear model that predicts a categorical label.\n",
      "3. **Decision Trees:** A tree-based model that predicts a categorical label.\n",
      "4. **Random Forest:** An ensemble model that combines multiple decision trees.\n",
      "5. **Support Vector Machines (SVMs):** A linear or non-linear model that predicts a categorical label.\n",
      "\n",
      "**Unsupervised Learning:**\n",
      "\n",
      "Unsupervised learning is a type of machine learning where the model is trained on unlabeled data. This means that the data does not have any inherent structure or meaning, and the model must discover patterns and relationships on its own.\n",
      "\n",
      "**Types of Unsupervised Learning:**\n",
      "\n",
      "1. **Clustering:** The model groups similar data points together.\n",
      "2. **Dimensionality Reduction:** The model reduces the number of features in the data.\n",
      "3. **Anomaly Detection:** The model identifies data points that are significantly different from the rest.\n",
      "\n",
      "**Common Unsupervised Learning Algorithms:**\n",
      "\n",
      "1. **K-Means Clustering:** A clustering algorithm that groups data points into clusters.\n",
      "2. **Hierarchical Clustering:** A clustering algorithm that builds a hierarchy of clusters.\n",
      "3. **Principal Component Analysis (PCA):** A dimensionality reduction algorithm that reduces the number of features.\n",
      "4. **t-Distributed Stochastic Neighbor Embedding (t-SNE):** A dimensionality reduction algorithm that reduces the number of features.\n",
      "\n",
      "**Real-World Applications:**\n",
      "\n",
      "1. **Image Classification:** Supervised learning is used to classify images into different categories.\n",
      "2. **Natural Language Processing (NLP):** Supervised learning is used to classify text into different categories.\n",
      "3. **Recommendation Systems:** Supervised learning is used to recommend products or services based on user behavior.\n",
      "4. **Anomaly Detection:** Unsupervised learning is used to detect unusual patterns in data.\n",
      "\n",
      "**Code Examples:**\n",
      "\n",
      "1. **Supervised Learning (Image Classification):**\n",
      "```python\n",
      "from tensorflow.keras.datasets import mnist\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.layers import Dense, Dropout\n",
      "\n",
      "# Load MNIST dataset\n",
      "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
      "\n",
      "# Reshape and normalize data\n",
      "x_train = x_train.reshape(-1, 784)\n",
      "x_test = x_test.reshape(-1, 784)\n",
      "x_train = x_train.astype('float32') / 255\n",
      "x_test = x_test.astype('float32') / 255\n",
      "\n",
      "# Define model architecture\n",
      "model = Sequential()\n",
      "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Dense(10, activation='softmax'))\n",
      "\n",
      "# Compile model\n",
      "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "# Train model\n",
      "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n",
      "\n",
      "# Evaluate model\n",
      "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
      "print(f'Test accuracy: {test_acc:.2f}')\n",
      "```\n",
      "\n",
      "2. **Unsupervised Learning (Clustering Customers):**\n",
      "```python\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.datasets import make_blobs\n",
      "\n",
      "# Generate sample data\n",
      "X, _ = make_blobs(n_samples=100, centers=5, n_features=2, random_state=0)\n",
      "\n",
      "# Define clustering model\n",
      "kmeans = KMeans(n_clusters=5, random_state=0)\n",
      "\n",
      "# Fit model\n",
      "kmeans.fit(X)\n",
      "\n",
      "# Get cluster labels\n",
      "labels = kmeans.labels_\n",
      "\n",
      "# Print cluster labels\n",
      "print(labels)\n",
      "```\n",
      "\n",
      "Let me know if you have any further questions or if there's anything else I can help you with!\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Suppose `result` is what you got from app.invoke(...)\n",
    "for msg in result[\"messages\"]:\n",
    "    # Check if the message has 'content'\n",
    "    if hasattr(msg, 'content'):\n",
    "        print(f\"{msg.name}:\")  # Optional: show which agent\n",
    "        print(msg.content)\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559408d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28716fe2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "029b4432",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = create_supervisor(\n",
    "    [\n",
    "        curriculum_agent, debug_agent,\n",
    "        resume_agent, unified_agent, news_agent\n",
    "    ],\n",
    "    model=hf_model,\n",
    "    prompt=\"\"\"\n",
    "You are SUPERVISOR-GUIDE, responsible for routing user queries to the appropriate specialized agent\n",
    "(curriculum, debugging, resume/cover letter, feedback, tech news) and producing a final, \n",
    "well-formatted, concise, and complete response.\n",
    "\n",
    "**Important:** For **any educational, conceptual, or curriculum-related query**, you must:\n",
    "\n",
    "1. Provide a step-by-step explanation with examples.\n",
    "2. Dynamically fetch up to 3 relevant YouTube videos using the `youtube_search` tool.\n",
    "3. Include **only valid video titles and URLs** returned by the `youtube_search` tool.\n",
    "4. If no videos are found, include this message: \"No relevant video found; refer to online resources.\"\n",
    "5. Format explanations clearly using bullet points, numbered lists, or code blocks.\n",
    "\n",
    "Follow these rules:\n",
    "\n",
    "1. **Identify intent**: Analyze the query and route it to the correct agent(s). Use multiple agents sequentially if needed.\n",
    "\n",
    "2. **Curriculum / Learning queries**:\n",
    "   - Fetch top relevant courses dynamically from websites.\n",
    "   - Attach YouTube video links using `youtube_search`.\n",
    "   - Present in **clean, point-wise format**: Course Name ‚Äì Video Link\n",
    "\n",
    "3. **Educational / Conceptual queries**:\n",
    "   - Forward to `debug_agent` or handle directly.\n",
    "   - Include step-by-step explanations and examples.\n",
    "   - Attach **YouTube videos dynamically** using `youtube_search`.\n",
    "\n",
    "4. **Debugging / Skills queries**: Forward to `debug_agent`. Include explanations, suggested fixes, examples, and exercises if relevant.\n",
    "\n",
    "5. **Resume / Cover letter queries**: Forward to `resume_agent`. Ensure output is professional, concise, and structured (HTML or readable text).\n",
    "\n",
    "6. **Feedback / Reviews**: Forward to `feedback_agent`. Responses should be friendly, motivational, and context-aware.\n",
    "\n",
    "7. **Technology News**: Forward to `news_agent`. Summarize in **point-wise format**, including trending tools or technologies. Keep it concise and actionable.\n",
    "\n",
    "8. **Formatting & Clarity**:\n",
    "   - Return **only the final output**; do not include internal agent steps.\n",
    "   - Include lists and links **only if valid**.\n",
    "   - Ensure readability: not too short, not too long.\n",
    "\n",
    "9. **Dynamic Handling**:\n",
    "   - Prefer real-time data retrieval via web search or APIs.\n",
    "   - Fall back gracefully if data is unavailable.\n",
    "\n",
    "Goal: Provide **accurate, complete, user-friendly, and visually clear responses** while coordinating all agents efficiently.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac90ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae13a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd24c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c7b2094a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'raw_info': 'Published year: 2023\\nTitle: One Hundred Most-cited Papers on Bacterial Meningitis: A Bibliometric Study\\nAuthors: Vinayak P Hakkaraki\\nAbstract: Background: In previous decades, large-scale research has been carried out on bacterial meningitis. In every field, citation analysis is the most significant contribution. The study‚Äôs objective was to identify and analyze the 100 articles on bacterial meningitis that received the most citations between 2000 and 2023, highlighting the most significant developments in the field. Objective: The objective of this study was to find out what makes a highly influential article by identifying and analyzing the characteristics of the 100 articles in the field of bacterial meningitis that receive the most citations. The goal of this study was to find and examine the 100 articles on bacterial meningitis that received the most citations. Methodology: We identified the top 100 most-cited papers in the field of bacterial meningitis from 55 journals using the Dimensions AI database. The results of each author‚Äôs analysis of 100 articles were then compared. We gathered fundamental data such as the journal‚Äôs title, country of publication, and study type. Descriptive counts or percentages were used to compare the various categories. Results: Between the year 2000 and the year 2023, articles were published. The total number of citations ranged from 115 to 1176, with 42 papers receiving more than 200 citations. In 2008, 14 articles were published, followed by 10 in 2000 and 2007. One thousand one hundred and seventy-six times were given to the most-cited paper, whereas 115 times were given to the least-cited article. ‚ÄúClinical Features and Prognostic Factors in Adults with Bacterial Meningitis,‚Äù by Diederik van de Beek, et al. (2004) was the article that received the most citations. 1176 people have cited this article. van de Beek Diederik of the Academic Medical Center in The Netherlands is the author who has written the most articles, was mentioned in 14 of the top 100 articles. Papers were primarily published in Pediatrics (n = 9) publication with 1861 citations. The Netherlands came in second with 18 publications, followed by the United States (n = 46). Conclusion: Our study uses bibliometrics and visualization analysis of the most important articles in this field to show the current state of research in the area of bacterial meningitis, provide a history of research trends, and offer a perspective for future bacterial predicts the growth of meningitis.',\n",
       "  'summary': \"This bibliometric study analyzes the top 100 most-cited articles about bacterial meningitis between 2000 and 2023, providing insights into the field's key developments. Through examining citation trends, authors found that influential articles focused on clinical features and prognostic factors, emphasizing the common themes in this area of research. The study also highlights that research on pediatric patients and articles published in influential journals have driven major advancements in understanding bacterial meningitis. \\n\"},\n",
       " {'raw_info': '\\nPublished year: 2023\\nTitle: Scientometric assessment of funded scientometrics and bibliometrics research (2011‚Äì2021)\\nAuthors: M. Verma, Daud Khan, M. Yuvaraj\\nAbstract: None',\n",
       "  'summary': 'This paper uses bibliometric and Scientometric analysis to assess the growth and evolution of funded research in scientometrics and bibliometrics between 2011 and 2021. The researchers aim to understand the trends, key areas of focus, and potential developments within this specialized field.  They present a comprehensive overview of the evolving trends and crucial contributions found in this research area. \\n'},\n",
       " {'raw_info': '\\nPublished year: 2023\\nTitle: Exploring the status of artificial intelligence for healthcare research in Africa: a bibliometric and thematic analysis\\nAuthors: Tabu S. Kondo, Salim Diwani, Ally S. Nyamawe, Mohamedi M. Mjahidi\\nAbstract: This paper explores the status of Artificial Intelligence (AI) for healthcare research in Africa. The aim was to use bibliometric and thematic analysis methods to determine the publication counts, leading authors, top journals and publishers, most active institutions and countries, most cited institutions, funding bodies, top subject areas, co-occurrence of keywords and co-authorship. Bibliographic data were collected on April 9 2022, through the Lens database, based on the critical areas of authorship studies, such as authorship pattern, number of authors, etc. The findings showed that several channels were used to disseminate the publications, including articles, conference papers, reviews, and others. Publications on computer science topped the list of documented subject categories. The Annals of Tropical Medicine and Public Health is the top journal, where articles on AI have been published. One of the top nations that published AI research was the United Kingdom. With 143 publications, Harvard University was the higher education institution that p',\n",
       "  'summary': 'This bibliometric and thematic analysis of healthcare research publications in AI from 2022 reveals the landscape of AI research in Africa. Findings show that computer science-focused content is dominant, with significant publications emanating from the UK and institutions like Harvard University. The analysis highlights diverse dissemination platforms and key regions driving AI research in Africa.  \\n'}]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# PDF Extraction Function\n",
    "# -------------------------\n",
    "def extract_text_from_pdf(url: str, max_pages: int = 5) -> str:\n",
    "    try:\n",
    "        response = requests.get(url, timeout=15)\n",
    "        if response.status_code != 200:\n",
    "            return \"\"\n",
    "        pdf_file = BytesIO(response.content)\n",
    "        reader = PdfReader(pdf_file)\n",
    "        text = [page.extract_text() or \"\" for page in reader.pages[:max_pages]]\n",
    "        return \"\\n\".join(text)\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting PDF: {e}\"\n",
    "\n",
    "# -------------------------\n",
    "# Initialize Semantic Scholar API\n",
    "# -------------------------\n",
    "ss = SemanticScholarAPIWrapper(\n",
    "    top_k_results=5,  # Number of top papers to fetch\n",
    "    load_max_docs=5,  # Limit of loaded documents\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Function to fetch & summarize papers\n",
    "# -------------------------\n",
    "def fetch_and_summarize(query: str, summarize: bool = True) -> List[Dict]:\n",
    "    raw_results = ss.run(query)  # Returns string with paper info\n",
    "    # Normally, you would parse JSON if you need structured data\n",
    "    # For demonstration, we split papers by double newlines\n",
    "    papers = raw_results.split(\"\\n\\n\")\n",
    "    summarized_papers = []\n",
    "\n",
    "    for paper in papers:\n",
    "        # Optionally summarize abstract with LLM\n",
    "        summary = \"\"\n",
    "        if summarize and \"abstract:\" in paper.lower():\n",
    "            prompt = f\"Summarize this research abstract in 2-3 sentences:\\n\\n{paper}\"\n",
    "            summary = hf_model.invoke([HumanMessage(content=prompt)]).content\n",
    "        summarized_papers.append({\n",
    "            \"raw_info\": paper,\n",
    "            \"summary\": summary\n",
    "        })\n",
    "\n",
    "    return summarized_papers\n",
    "\n",
    "fetch_and_summarize(\"top AI research papers 2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "87bcfcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=' The main topic of this video is Pedro Pascal\\'s career, his experiences working on shows like \"The Last of Us,\" \"Game of Thrones,\" and \"The Mandalorian,\" as well as his upbringing in Chile and the United States.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 2206, 'total_tokens': 2263}, 'model_name': 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run--adae7a45-52bb-4f00-92d6-50074f8bb3a5-0' usage_metadata={'input_tokens': 2206, 'output_tokens': 57, 'total_tokens': 2263}\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled, VideoUnavailable\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize LLM\n",
    "# -----------------------------\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False,\n",
    "    temperature=0.1\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt Template\n",
    "# -----------------------------\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant designed to answer questions about a YouTube video based on its transcript.\n",
    "Answer the user's question using ONLY the provided transcript context.\n",
    "If the information is not in the context, explicitly say \"I cannot find information about that in the video transcript.\"\n",
    "\n",
    "Transcript:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
    "\n",
    "# -----------------------------\n",
    "# Transcript Fetcher\n",
    "# -----------------------------\n",
    "def get_transcript(video_id: str):\n",
    "    \"\"\"Fetch transcript safely for old and new versions of youtube_transcript_api.\"\"\"\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi().fetch(video_id, languages=[\"hi\"])\n",
    "    except NoTranscriptFound:\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi().fetch(video_id, languages=[\"en\"])\n",
    "        except NoTranscriptFound:\n",
    "            return None, \"No transcript available in Hindi or English.\"\n",
    "    except (TranscriptsDisabled, VideoUnavailable) as e:\n",
    "        return None, str(e)\n",
    "    except Exception as e:\n",
    "        return None, f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "    # Handle both dicts and FetchedTranscriptSnippet objects\n",
    "    texts = []\n",
    "    for snippet in transcript_list:\n",
    "        if isinstance(snippet, dict):\n",
    "            texts.append(snippet.get(\"text\", \"\"))\n",
    "        else:\n",
    "            # FetchedTranscriptSnippet object\n",
    "            texts.append(getattr(snippet, \"text\", \"\"))\n",
    "\n",
    "    transcript = \" \".join(texts)\n",
    "    return transcript, None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main YouTube QA / Summarizer\n",
    "# -----------------------------\n",
    "def youtube_qa(video_url: str, question: str):\n",
    "    \"\"\"Given a YouTube URL and question, return answer based on transcript.\"\"\"\n",
    "    try:\n",
    "        video_id = video_url.split(\"v=\")[-1].split(\"&\")[0]  # Extract only the video ID\n",
    "        transcript, error = get_transcript(video_id)\n",
    "        if error:\n",
    "            return error\n",
    "\n",
    "        rag_runnable = (\n",
    "            {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | model\n",
    "        )\n",
    "        answer = rag_runnable.invoke({\"context\": transcript, \"question\": question})\n",
    "        return answer\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "video_url = \"https://www.youtube.com/watch?v=QsYGlZkevEg\"\n",
    "question = \"What is the main topic of this video?\"\n",
    "print(youtube_qa(video_url, question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5c29fd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-query: Explain 'Attention Mechanism in Transformers' using both papers and video\n",
      "Routed to: youtube_search_agent\n",
      "Attention in transformers, step-by-step | Deep Learning Chapter 6 - https://www.youtube.com/watch?v=eMlx5fFNoYc\n",
      "Attention mechanism: Overview - https://www.youtube.com/watch?v=fjJOgb-E41w\n",
      "Vision transformers #machinelearning #datascience #computervision - https://www.youtube.com/watch?v=qPUYBX0C6ic\n",
      "------------------------------------------------------------\n",
      "Sub-query: Summarize latest AI news and trends today\n",
      "Routed to: debug_agent\n",
      "Summarize latest AI news and trends today\n",
      " Today, I'd like to share some of the latest AI news and trends that have caught my attention:\n",
      "\n",
      "1. OpenAI's DALL-E 2: OpenAI has unveiled an updated version of its AI-powered image generator, DALL-E 2. The new model can create highly detailed and visually stunning images based on text descriptions, showcasing the impressive progress in AI-generated art.\n",
      "\n",
      "2. Google's Bard AI: Google has announced its new AI model, Bard, which aims to provide more conversational and helpful responses to users' questions. Bard is designed to be more accessible and user-friendly, and it's expected to be integrated into various Google services in the future.\n",
      "\n",
      "3. AI in healthcare: AI is increasingly being used in the healthcare industry to improve diagnosis, treatment, and patient care. For example, a new AI system developed by researchers at the University of California, San Francisco, can accurately predict which patients are at high risk of developing sepsis, a life-threatening condition.\n",
      "\n",
      "4. Ethical concerns about AI: As AI continues to advance, there are growing concerns about its potential impact on society, including issues related to privacy, bias, and job displacement. Governments and organizations around the world are working to develop guidelines and regulations to address these concerns and ensure that AI is developed and used responsibly.\n",
      "\n",
      "5. AI in finance: AI is being used in the finance industry to improve risk management, fraud detection, and investment strategies. For example, a new AI model developed by researchers at the University of Cambridge can predict stock market movements with remarkable accuracy, potentially revolutionizing the way investments are made.\n",
      "\n",
      "6. AI in education: AI is being used in education to personalize learning experiences, identify areas where students may be struggling, and provide additional support. For example, a new AI-powered tutoring system developed by Carnegie Learning can adapt to each student's learning style and provide personalized feedback to help them improve.\n",
      "\n",
      "7. AI in agriculture: AI is being used in agriculture to optimize crop yields, improve resource management, and reduce waste. For example, a new AI system developed by researchers at the University of California, Davis, can predict crop yields based on weather patterns, soil conditions, and other factors, helping farmers make more informed decisions about planting and harvesting.\n",
      "\n",
      "8. AI in transportation: AI is being used in transportation to improve safety, efficiency, and convenience. For example, self-driving cars are becoming more common, and AI is being used to optimize traffic flow and reduce congestion.\n",
      "\n",
      "Overall, AI is rapidly advancing and is being applied in a wide range of industries and applications. As with any technology, it's important to consider the potential benefits and risks, and to ensure that it is developed and used in a way that is ethical, responsible, and beneficial to society as a whole.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import List, Dict\n",
    "import re, requests\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.utilities.semanticscholar import SemanticScholarAPIWrapper\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled, VideoUnavailable\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Initialize LLM\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Tools\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "API_KEY = \"AIzaSyBNBTgze_5FR5VHzfZlxc38iLwr7xyYaHE\"\n",
    "# def youtube_search_guaranteed(query: str, max_results=3) -> List[str]:\n",
    "#     search_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "#     params = {\"part\":\"snippet\",\"q\":query,\"type\":\"video\",\"maxResults\":20,\"key\":API_KEY}\n",
    "#     items = requests.get(search_url, params=params).json().get(\"items\", [])\n",
    "#     video_ids = [i['id']['videoId'] for i in items if i.get('id', {}).get('videoId')]\n",
    "#     if not video_ids: return []\n",
    "\n",
    "#     details_url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "#     params = {\"part\":\"status,snippet\",\"id\":\",\".join(video_ids),\"key\":API_KEY}\n",
    "#     details_res = requests.get(details_url, params=params).json()\n",
    "#     videos = []\n",
    "#     for v in details_res.get(\"items\", []):\n",
    "#         s = v.get(\"status\", {})\n",
    "#         if s.get(\"uploadStatus\") != \"processed\" or s.get(\"privacyStatus\") != \"public\":\n",
    "#             continue\n",
    "#         snip = v.get(\"snippet\", {})\n",
    "#         videos.append(f\"{snip.get('title')} - https://www.youtube.com/watch?v={v.get('id')}\")\n",
    "#         if len(videos)>=max_results: break\n",
    "#     return videos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def youtube_search(query: str) -> str:\n",
    "    \"\"\"Search YouTube for videos related to the query and return video links.\"\"\"\n",
    "    import requests\n",
    "    API_KEY = \"AIzaSyBNBTgze_5FR5VHzfZlxc38iLwr7xyYaHE\"\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/search?part=snippet&q={query}&type=video&maxResults=3&key={API_KEY}\"\n",
    "    res = requests.get(url).json()\n",
    "    videos = [f\"https://www.youtube.com/watch?v={item['id']['videoId']}\" for item in res.get(\"items\", [])]\n",
    "    return \"\\n\".join(videos) if videos else \"No related videos found.\"\n",
    "\n",
    "@tool\n",
    "def topic_explanation(query: str) -> str:\n",
    "    \"\"\" Return a concise conceptual explanation of the topic. \"\"\"\n",
    "    prompt = f\"Explain '{query}' in 3-5 sentences, beginner-friendly.\"\n",
    "    return hf_model.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "@tool\n",
    "def generate_resume(job_description: str, candidate_info: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a professional LaTeX resume and cover letter.\n",
    "\n",
    "    Args:\n",
    "        job_description (str): Target job description or role.\n",
    "        candidate_info (str, optional): Candidate's info (skills, experience, education).\n",
    "\n",
    "    Returns:\n",
    "        str: LaTeX-formatted resume + cover letter (Overleaf style).\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a professional CV/Resume creator.\n",
    "    Generate a **LaTeX resume** and cover letter using the `moderncv` style\n",
    "    (clean, Overleaf-friendly).\n",
    "\n",
    "    Requirements:\n",
    "    - Must be compile-ready LaTeX code.\n",
    "    - Include:\n",
    "      * Header: Name, Contact (email, phone, LinkedIn, GitHub)\n",
    "      * Summary\n",
    "      * Skills\n",
    "      * Education\n",
    "      * Experience / Projects\n",
    "      * Achievements\n",
    "    - After resume, include a cover letter with proper LaTeX letter formatting.\n",
    "    - Do NOT explain anything, return ONLY valid LaTeX code.\n",
    "\n",
    "    Job Description:\n",
    "    {job_description}\n",
    "\n",
    "    Candidate Info:\n",
    "    {candidate_info if candidate_info else \"General fresher/student profile\"}\n",
    "    \"\"\"\n",
    "    return hf_model.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ss = SemanticScholarAPIWrapper(top_k_results=5, load_max_docs=5)\n",
    "\n",
    "@tool\n",
    "def semantic_scholar_research(query: str, summarize: bool=True) -> List[Dict]:\n",
    "    \"\"\" Fetch top research papers from Semantic Scholar and optionally summarize abstracts. \"\"\"\n",
    "    raw = ss.run(query)\n",
    "    papers = raw.split(\"\\n\\n\")\n",
    "    result=[]\n",
    "    for p in papers:\n",
    "        summary=\"\"\n",
    "        if summarize and \"abstract:\" in p.lower():\n",
    "            summary=hf_model.invoke([HumanMessage(content=f\"Summarize in 2-3 sentences:\\n{p}\")]).content\n",
    "        result.append({\"raw_info\":p,\"summary\":summary})\n",
    "    return result\n",
    "\n",
    "yt_prompt_template = \"\"\"\n",
    "Transcript:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "yt_prompt = PromptTemplate(template=yt_prompt_template, input_variables=[\"context\",\"question\"])\n",
    "\n",
    "def get_transcript(video_id: str):\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi().fetch(video_id, languages=[\"hi\"])\n",
    "    except NoTranscriptFound:\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi().fetch(video_id, languages=[\"en\"])\n",
    "        except NoTranscriptFound: return None, \"No transcript\"\n",
    "    except (TranscriptsDisabled, VideoUnavailable) as e: return None, str(e)\n",
    "    texts = [snippet.get(\"text\",\"\") if isinstance(snippet, dict) else getattr(snippet,\"text\",\"\") for snippet in transcript_list]\n",
    "    return \" \".join(texts), None\n",
    "\n",
    "@tool\n",
    "def youtube_qa(video_url:str, question:str):\n",
    "    \"\"\" Given a YouTube URL and question, return answer based on transcript. \"\"\"\n",
    "    vid_id=video_url.split(\"v=\")[-1].split(\"&\")[0]\n",
    "    transcript,_=get_transcript(vid_id)\n",
    "    rag = ({\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()} | yt_prompt | hf_model)\n",
    "    return rag.invoke({\"context\": transcript, \"question\": question})\n",
    "\n",
    "duck_tool = DuckDuckGoSearchRun()\n",
    "tavily_tool = TavilySearch()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Agents\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "curriculum_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[topic_explanation, youtube_search_agent],\n",
    "    name=\"curriculum_agent\",\n",
    "    prompt=\"You are CURRICULUM-GUIDE. Use topic_explanation + youtube_search_agent. just \" \\\n",
    "    \"when you call ` youtube_search_agent` then it should send query as {topic} so that it can fetch relevant videos.   \" \\\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "debug_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[youtube_search_agent, duck_tool, tavily_tool],\n",
    "    name=\"debug_agent\",\n",
    "    prompt=\"You are CODE-TUTOR. Debug and review resumes.\"\n",
    ")\n",
    "\n",
    "resume_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[generate_resume],\n",
    "    name=\"resume_agent\",\n",
    "    prompt=\"You are RESUME CREATOR. Generate resumes.\"\n",
    ")\n",
    "\n",
    "news_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[tavily_tool, duck_tool, youtube_search_agent],\n",
    "    name=\"news_agent\",\n",
    "    prompt=\"You are TECH-TREND-GUIDE. Summarize latest tech news.\"\n",
    ")\n",
    "\n",
    "unified_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[youtube_qa, semantic_scholar_research],\n",
    "    name=\"unified_agent\",\n",
    "    prompt=\"Answer YouTube QA & research papers.\"\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Dynamic Intent Router\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# import re\n",
    "\n",
    "# def route_query(query: str):\n",
    "#     \"\"\"\n",
    "#     Dynamically routes user queries to the correct agent using \n",
    "#     a weighted scoring system with tie-breaking.\n",
    "    \n",
    "#     If query mentions both education and video, video gets priority.\n",
    "#     \"\"\"\n",
    "#     query_lower = query.lower()\n",
    "\n",
    "#     # Define keyword sets for each intent\n",
    "#     intent_keywords = {\n",
    "#         \"curriculum\": [\n",
    "#             \"explain\", \"difference between\", \"teach\", \"guide\", \"tutorial\",\n",
    "#             \"lesson\", \"steps\", \"how to\", \"define\", \"meaning of\", \"concept\"\n",
    "#         ],\n",
    "#         \"video\": [\n",
    "#             \"summarize this video\", \"video summary\", \"explain video\",\n",
    "#             \"youtube summary\", \"summarize yt\", \"video link\", \"show video\"\n",
    "#         ],\n",
    "#         \"resume_create\": [\n",
    "#             \"create resume\", \"make resume\", \"build resume\", \"generate resume\",\n",
    "#             \"draft resume\", \"prepare resume\", \"design resume\", \"write resume\",\n",
    "#             \"produce resume\", \"craft resume\", \"develop resume\"\n",
    "#         ],\n",
    "#         \"resume_review\": [\n",
    "#             \"review resume\", \"improve resume\", \"fix resume\", \"debug resume\",\n",
    "#             \"correct resume\", \"optimize resume\", \"enhance resume\", \"edit resume\",\n",
    "#             \"update resume\", \"polish resume\", \"refine resume\", \"analyze resume\"\n",
    "#         ],\n",
    "#         \"research\": [\n",
    "#             \"research paper\", \"summarize research\", \"academic\", \"journal\",\n",
    "#             \"study summary\", \"paper review\", \"literature review\"\n",
    "#         ],\n",
    "#         \"news\": [\n",
    "#             \"trend\", \"latest news\", \"breaking\", \"headlines\",\n",
    "#             \"today news\", \"current affairs\", \"updates\"\n",
    "#         ],\n",
    "#     }\n",
    "\n",
    "#     # Initialize scores\n",
    "#     scores = {intent: 0 for intent in intent_keywords.keys()}\n",
    "\n",
    "#     # Score calculation\n",
    "#     for intent, keywords in intent_keywords.items():\n",
    "#         for kw in keywords:\n",
    "#             if re.search(rf\"\\b{kw}\\b\", query_lower):\n",
    "#                 scores[intent] += 1\n",
    "\n",
    "#     # Special case: YouTube links ‚Üí strong video signal\n",
    "#     if re.search(r\"https?://.*youtube.*\", query_lower):\n",
    "#         scores[\"video\"] += 5  # weighted boost\n",
    "\n",
    "#     # Pick best matching agent\n",
    "#     best_intent = max(scores, key=scores.get)\n",
    "#     best_score = scores[best_intent]\n",
    "\n",
    "#     # Tie-break: if both curriculum & video are > 0\n",
    "#     if scores[\"curriculum\"] > 0 and scores[\"video\"] > 0:\n",
    "#         # If user explicitly asks for video ‚Üí route to unified_agent\n",
    "#         if any(word in query_lower for word in [\"video\", \"youtube\", \"yt\", \"link\"]):\n",
    "#             best_intent = \"video\"\n",
    "#         else:\n",
    "#             best_intent = \"curriculum\"\n",
    "\n",
    "#     # Map intent to agent\n",
    "#     if best_score == 0:\n",
    "#         return debug_agent  # fallback\n",
    "#     elif best_intent == \"curriculum\":\n",
    "#         return curriculum_agent\n",
    "#     elif best_intent == \"video\":\n",
    "#         return unified_agent\n",
    "#     elif best_intent == \"resume_create\":\n",
    "#         return resume_agent\n",
    "#     elif best_intent == \"resume_review\":\n",
    "#         return debug_agent\n",
    "#     elif best_intent == \"research\":\n",
    "#         return unified_agent\n",
    "#     elif best_intent == \"news\":\n",
    "#         return news_agent\n",
    "#     else:\n",
    "#         return debug_agent\n",
    "\n",
    "\n",
    "workflow = create_supervisor(\n",
    "    [curriculum_agent, debug_agent, resume_agent, unified_agent, news_agent],\n",
    "    model=hf_model,\n",
    "    prompt=\"SUPERVISOR: Route dynamically based on intent.\"\n",
    ")\n",
    "app = workflow.compile()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Example Run\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import re\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# ------------------------\n",
    "# Multi-query splitter\n",
    "# ------------------------\n",
    "def split_multitask_query(query: str):\n",
    "    \"\"\"\n",
    "    Split a query into multiple sub-queries based on line breaks, semicolons, or colons.\n",
    "    \"\"\"\n",
    "    sub_queries = re.split(r\"\\n|;|:\", query)\n",
    "    return [q.strip() for q in sub_queries if q.strip()]\n",
    "\n",
    "# ------------------------\n",
    "# Existing intent-based routing\n",
    "# ------------------------\n",
    "def route_query(query: str):\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    # 1Ô∏è‚É£ Handle explicit YouTube/video queries first\n",
    "    if any(word in query_lower for word in [\"youtube\", \"video\", \"yt\", \"show\", \"explain video\"]):\n",
    "        return youtube_search_agent\n",
    "\n",
    "    # 2Ô∏è‚É£ Handle research paper queries explicitly\n",
    "    if \"summarize\" in query_lower and \"paper\" in query_lower:\n",
    "        return unified_agent\n",
    "\n",
    "    # 3Ô∏è‚É£ Keyword-based scoring for other intents\n",
    "    intent_keywords = {\n",
    "        \"curriculum\": [\"explain\", \"difference between\", \"teach\", \"guide\", \"tutorial\", \"lesson\", \"steps\", \"how to\", \"define\", \"meaning of\", \"concept\"],\n",
    "        \"resume_create\": [\"create resume\", \"make resume\", \"build resume\", \"generate resume\", \"draft resume\", \"prepare resume\", \"design resume\", \"write resume\"],\n",
    "        \"resume_review\": [\"review resume\", \"improve resume\", \"fix resume\", \"debug resume\", \"correct resume\", \"optimize resume\", \"enhance resume\", \"edit resume\", \"update resume\", \"polish resume\", \"refine resume\", \"analyze resume\"],\n",
    "        \"research\": [\"research paper\", \"summarize research\", \"academic\", \"journal\", \"study summary\", \"paper review\", \"literature review\"],\n",
    "        \"news\": [\"trend\", \"latest news\", \"breaking\", \"headlines\", \"today news\", \"current affairs\", \"updates\"],\n",
    "    }\n",
    "\n",
    "    scores = {intent: 0 for intent in intent_keywords.keys()}\n",
    "    for intent, keywords in intent_keywords.items():\n",
    "        for kw in keywords:\n",
    "            if re.search(rf\"\\b{kw}\\b\", query_lower):\n",
    "                scores[intent] += 1\n",
    "\n",
    "    best_intent = max(scores, key=scores.get)\n",
    "    best_score = scores[best_intent]\n",
    "\n",
    "    # Map to agent\n",
    "    if best_score == 0:\n",
    "        return debug_agent\n",
    "    elif best_intent == \"curriculum\":\n",
    "        return curriculum_agent\n",
    "    elif best_intent == \"resume_create\":\n",
    "        return resume_agent\n",
    "    elif best_intent == \"resume_review\":\n",
    "        return debug_agent\n",
    "    elif best_intent == \"research\":\n",
    "        return unified_agent\n",
    "    elif best_intent == \"news\":\n",
    "        return news_agent\n",
    "    else:\n",
    "        return debug_agent\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Multi-query executor\n",
    "# ------------------------\n",
    "# ------------------------\n",
    "# Multi-query executor with guaranteed YouTube search\n",
    "# ------------------------\n",
    "def execute_multi_query(full_query: str):\n",
    "    \"\"\"\n",
    "    Accepts a multi-task query (with ; or newlines), splits it,\n",
    "    routes each sub-query, and executes it with the correct agent.\n",
    "    \n",
    "    YouTube/video queries are guaranteed to hit the real YouTube search tool.\n",
    "    \"\"\"\n",
    "    sub_queries = split_multitask_query(full_query)\n",
    "    results = []\n",
    "\n",
    "    for q in sub_queries:\n",
    "        q_lower = q.lower()\n",
    "        # Check if query is likely a YouTube/video query\n",
    "        if \"youtube\" in q_lower or \"video\" in q_lower or \"show\" in q_lower or \"explain\" in q_lower:\n",
    "            # Call the YouTube search tool directly\n",
    "            output = youtube_search_agent(q)\n",
    "            results.append((q, \"youtube_search_agent\", [output]))\n",
    "            continue\n",
    "\n",
    "        # Otherwise, route normally\n",
    "        agent = route_query(q)\n",
    "        response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": q}]})\n",
    "\n",
    "        # Process agent messages\n",
    "        outputs = []\n",
    "        for msg in response[\"messages\"]:\n",
    "            text = msg.content\n",
    "\n",
    "            # Detect if agent \"simulated\" a YouTube call\n",
    "            # Example: it outputs {query} inside the text\n",
    "            m = re.search(r\"\\{(.+?)\\}\", text)\n",
    "            if \"youtube_search_agent\" in text.lower() and m:\n",
    "                # Replace placeholder with real search\n",
    "                query_for_tool = m.group(1)\n",
    "                text = youtube_search_agent(query_for_tool)\n",
    "\n",
    "            outputs.append(text)\n",
    "\n",
    "        results.append((q, agent.name, outputs))\n",
    "\n",
    "    return results\n",
    "\n",
    "# ...existing code...\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# ------------------------\n",
    "# Example Usage\n",
    "# ------------------------\n",
    "queries = [\n",
    "    \"Explain 'Attention Mechanism in Transformers' using both papers and video;\"\n",
    "    \"Summarize latest AI news and trends today;\"\n",
    "    \"Difference between CNN and RNN and show related YouTube video;\"\n",
    "]\n",
    "\n",
    "# for q in queries:\n",
    "#     results = execute_multi_query(q)\n",
    "#     for sub_q, agent_name, outputs in results:\n",
    "#         print(f\"Sub-query: {sub_q}\\nRouted to: {agent_name}\")\n",
    "#         for out in outputs:\n",
    "#             print(out)\n",
    "#         print(\"-\"*60)\n",
    "queries = [\n",
    "    \"Explain 'Attention Mechanism in Transformers' using both papers and video ; \"\n",
    "    \"Summarize latest AI news and trends today\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    results = execute_multi_query(q)\n",
    "    for sub_q, agent_name, outputs in results:\n",
    "        print(f\"Sub-query: {sub_q}\\nRouted to: {agent_name}\")\n",
    "        for out in outputs:\n",
    "            print(out)\n",
    "        print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d961dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SemanticScholarAPIWrapper(top_k_results=5, load_max_docs=5)\n",
    "\n",
    "@tool\n",
    "def semantic_scholar_research(query: str, summarize: bool=True) -> List[Dict]:\n",
    "    \"\"\" Fetch top research papers from Semantic Scholar and optionally summarize abstracts. \"\"\"\n",
    "    raw = ss.run(query)\n",
    "    papers = raw.split(\"\\n\\n\")\n",
    "    result=[]\n",
    "    for p in papers:\n",
    "        summary=\"\"\n",
    "        if summarize and \"abstract:\" in p.lower():\n",
    "            summary=hf_model.invoke([HumanMessage(content=f\"Summarize in 2-3 sentences:\\n{p}\")]).content\n",
    "        result.append({\"raw_info\":p,\"summary\":summary})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7f4bcd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'raw_info': 'No results found.', 'summary': ''}]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_scholar_research.invoke(\"give me the summary of top 5 rsearch ppaer on Attention is all you need \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e99727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-query: Explain 'Attention Mechanism in Transformers' using both papers and video\n",
      "Outputs:\n",
      "Explain 'Attention Mechanism in Transformers' using both papers and video\n",
      " The Attention Mechanism in Transformers is a crucial component in the Transformer model, which was introduced in the paper \"Attention is All You Need\" by Vaswani et al. (2017). This mechanism allows the model to focus on relevant parts of the input sequence when generating an output, making it more efficient and effective, especially for long sequences.\n",
      "\n",
      "In the Transformer model, the Attention Mechanism is used in both the Encoder and Decoder. It works by assigning different weights (or scores) to each input token based on their relevance to the current output token. These weights are calculated using a dot product of the input and query vectors, followed by a softmax function to ensure the weights sum to 1.\n",
      "\n",
      "The query, key, and value vectors are derived from the input sequence. The query and key vectors are used to calculate the attention scores, while the value vectors provide the actual output based on the attended inputs.\n",
      "\n",
      "The paper provides a detailed explanation of the Scaled Dot-Product Attention and Multi-Head Attention mechanisms, which are the building blocks of the Attention Mechanism in Transformers.\n",
      "\n",
      "Here's a simplified version of the Scaled Dot-Product Attention:\n",
      "\n",
      "1. Calculate the attention scores (QK' transpose) for each input token, where Q and K are the query and key vectors, respectively.\n",
      "2. Apply the softmax function to the attention scores to get the normalized weights.\n",
      "3. Multiply the normalized weights with the value vectors to get the weighted sum of the values.\n",
      "4. Scale the weighted sum by a factor of sqrt(d_k) to prevent the gradients from vanishing.\n",
      "\n",
      "The Multi-Head Attention mechanism works by applying the Scaled Dot-Product Attention multiple times with different linear projections (or transformations) of the query, key, and value vectors. This allows the model to attend to information from different perspectives simultaneously.\n",
      "\n",
      "For a more visual understanding, I recommend watching the following video by 3Blue1Brown, which provides a clear and intuitive explanation of the Attention Mechanism in Transformers:\n",
      "\n",
      "[3Blue1Brown: Attention Mechanism in Transformers](https://www.youtube.com/watch?v=oJr_Qj-Zzoo)\n",
      "\n",
      "This video not only explains the Attention Mechanism in Transformers but also provides insights into why it's essential for handling long sequences and how it compares to the Recurrent Neural Network (RNN) approach.\n",
      "------------------------------------------------------------\n",
      "Sub-query: Summarize latest AI news and trends today\n",
      "Outputs:\n",
      "Summarize latest AI news and trends today\n",
      " To dynamically decide which agent should handle a user query about the latest AI news and trends today, we can consider the following factors:\n",
      "\n",
      "1. Content: The agent with expertise in AI news and trends should be selected. This agent should be able to provide up-to-date information about the latest developments in the field, including breakthroughs, research findings, and industry news.\n",
      "\n",
      "2. Context: If the user has previously shown interest in a specific subfield of AI, such as machine learning or natural language processing, the agent with expertise in that area should be prioritized.\n",
      "\n",
      "3. Type of request: If the user's query is more general, an agent with a broader understanding of AI and its applications would be more suitable. If the user's query is more specific, an agent with expertise in that particular area would be more appropriate.\n",
      "\n",
      "In this case, the best agent to handle the user query would be the one that specializes in AI news and trends, has up-to-date knowledge, and can provide a comprehensive response to the user's query. This agent should be able to dynamically adapt to the user's needs and preferences, ensuring that they receive the most relevant and accurate information.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.utilities.semanticscholar import SemanticScholarAPIWrapper\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_supervisor import create_supervisor\n",
    "import requests\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_model = ChatHuggingFace(model_name=\"gpt-4\")\n",
    "API_KEY = \"AIzaSyBNBTgze_5FR5VHzfZlxc38iLwr7xyYaHE\"\n",
    "\n",
    "# ------------------------\n",
    "# Tools\n",
    "# ------------------------\n",
    "@tool\n",
    "def youtube_search(query: str, max_results: int = 3) -> List[str]:\n",
    "    \"\"\"Guaranteed YouTube search returning public processed videos.\"\"\"\n",
    "    search_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "    params = {\"part\": \"snippet\", \"q\": query, \"type\": \"video\", \"maxResults\": 20, \"key\": API_KEY}\n",
    "    items = requests.get(search_url, params=params).json().get(\"items\", [])\n",
    "    video_ids = [i['id']['videoId'] for i in items if i.get('id', {}).get('videoId')]\n",
    "    if not video_ids: return [\"No related videos found.\"]\n",
    "    details_url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "    params = {\"part\": \"status,snippet\", \"id\": \",\".join(video_ids), \"key\": API_KEY}\n",
    "    details_res = requests.get(details_url, params=params).json()\n",
    "    videos = []\n",
    "    for v in details_res.get(\"items\", []):\n",
    "        status = v.get(\"status\", {})\n",
    "        if status.get(\"uploadStatus\") != \"processed\" or status.get(\"privacyStatus\") != \"public\":\n",
    "            continue\n",
    "        snip = v.get(\"snippet\", {})\n",
    "        videos.append(f\"{snip.get('title')} - https://www.youtube.com/watch?v={v.get('id')}\")\n",
    "        if len(videos) >= max_results:\n",
    "            break\n",
    "    return videos if videos else [\"No related videos found.\"]\n",
    "\n",
    "@tool\n",
    "def topic_explanation(query: str) -> str:\n",
    "    prompt = f\"Explain '{query}' in 3-5 sentences, beginner-friendly.\"\n",
    "    return hf_model.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "@tool\n",
    "def generate_resume(job_description: str, candidate_info: str = \"\") -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Generate a professional LaTeX resume and cover letter for:\n",
    "    Job Description: {job_description}\n",
    "    Candidate Info: {candidate_info if candidate_info else \"General fresher/student profile\"}\n",
    "    \"\"\"\n",
    "    return hf_model.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "ss = SemanticScholarAPIWrapper(top_k_results=5, load_max_docs=5)\n",
    "\n",
    "@tool\n",
    "def semantic_scholar_research(query: str, summarize: bool=True) -> List[Dict]:\n",
    "    raw = ss.run(query)\n",
    "    papers = raw.split(\"\\n\\n\")\n",
    "    results = []\n",
    "    for p in papers:\n",
    "        summary = \"\"\n",
    "        if summarize and \"abstract:\" in p.lower():\n",
    "            summary = hf_model.invoke([HumanMessage(content=f\"Summarize in 2-3 sentences:\\n{p}\")]).content\n",
    "        results.append({\"raw_info\": p, \"summary\": summary})\n",
    "    return results\n",
    "\n",
    "tavily_tool = TavilySearch()\n",
    "duck_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "# ------------------------\n",
    "# Agents\n",
    "# ------------------------\n",
    "curriculum_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[topic_explanation, youtube_search],\n",
    "    name=\"curriculum_agent\",\n",
    "    prompt=\"CURRICULUM-GUIDE: Explain concepts using topic_explanation or YouTube videos.\"\n",
    ")\n",
    "\n",
    "resume_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[generate_resume],\n",
    "    name=\"resume_agent\",\n",
    "    prompt=\"RESUME-CREATOR: Generate resumes and cover letters.\"\n",
    ")\n",
    "\n",
    "news_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[duck_tool, tavily_tool, youtube_search],\n",
    "    name=\"news_agent\",\n",
    "    prompt=\"TECH-TREND-GUIDE: Summarize latest tech news.\"\n",
    ")\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=hf_model,\n",
    "    tools=[semantic_scholar_research, youtube_search],\n",
    "    name=\"research_agent\",\n",
    "    prompt=\"RESEARCH-GUIDE: Answer academic queries using Semantic Scholar and YouTube if needed.\"\n",
    ")\n",
    "\n",
    "# ------------------------\n",
    "# Supervisor\n",
    "# ------------------------\n",
    "supervisor = create_supervisor(\n",
    "    agents=[curriculum_agent, resume_agent, research_agent, news_agent],\n",
    "    model=hf_model,\n",
    "    prompt=\"\"\"\n",
    "    You are a SUPERVISOR. Dynamically decide which agent(s) and tool(s) to use for any user query.\n",
    "    You can combine multiple agents if needed and ensure outputs are accurate.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "app = supervisor.compile()\n",
    "\n",
    "# ------------------------\n",
    "# Multi-query executor\n",
    "# ------------------------\n",
    "def split_multitask_query(query: str) -> list[str]:\n",
    "    \"\"\"Split multi-part queries automatically\"\"\"\n",
    "    sub_queries = re.split(r\"\\n|;|\\+|:\", query)\n",
    "    return [q.strip() for q in sub_queries if q.strip()]\n",
    "\n",
    "def execute_multi_query(full_query: str):\n",
    "    results = []\n",
    "    for q in split_multitask_query(full_query):\n",
    "        response = app.invoke({\"messages\": [{\"role\": \"user\", \"content\": q}]})\n",
    "        outputs = [m.content for m in response[\"messages\"]]\n",
    "        results.append({\"sub_query\": q, \"outputs\": outputs})\n",
    "    return results\n",
    "\n",
    "# ------------------------\n",
    "# Example Usage\n",
    "# ------------------------\n",
    "queries = [\n",
    "    \"Explain 'Attention Mechanism in Transformers' using papers and video + Summarize latest AI news and trends today\",\n",
    "    \"Generate a resume for a software engineer role\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    results = execute_multi_query(q)\n",
    "    for r in results:\n",
    "        print(f\"Sub-query: {r['sub_query']}\")\n",
    "        for out in r[\"outputs\"]:\n",
    "            print(out)\n",
    "        print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a3d24509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN vs CNN vs RNN | Difference Between ANN CNN and RNN | Types of Neural Networks Explained - https://www.youtube.com/watch?v=u7obuspdQu4\n",
      "ANN, CNN, DNN, RNN - What is the difference ü§Øü§Ø Easy explanation for beginners! Get started with ML - https://www.youtube.com/watch?v=8BUzHI3jfKI\n",
      "What are Convolutional Neural Networks (CNNs)? - https://www.youtube.com/watch?v=QzY57FaENXg\n"
     ]
    }
   ],
   "source": [
    "def youtube_search_guaranteed(query: str, max_results=3) -> List[str]:\n",
    "    search_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "    params = {\"part\": \"snippet\", \"q\": query, \"type\": \"video\", \"maxResults\": 20, \"key\": API_KEY}\n",
    "    items = requests.get(search_url, params=params).json().get(\"items\", [])\n",
    "\n",
    "    video_ids = []\n",
    "    for i in items:\n",
    "        vid = i.get('id', {}).get('videoId')\n",
    "        if vid and vid not in video_ids:  # avoid duplicates\n",
    "            video_ids.append(vid)\n",
    "        if len(video_ids) >= max_results*2:  # fetch extra to filter later\n",
    "            break\n",
    "\n",
    "    if not video_ids:\n",
    "        return []\n",
    "\n",
    "    # Fetch video details\n",
    "    details_url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "    params = {\"part\": \"status,snippet\", \"id\": \",\".join(video_ids), \"key\": API_KEY}\n",
    "    details_res = requests.get(details_url, params=params).json()\n",
    "\n",
    "    videos = []\n",
    "    for v in details_res.get(\"items\", []):\n",
    "        status = v.get(\"status\", {})\n",
    "        if status.get(\"uploadStatus\") != \"processed\" or status.get(\"privacyStatus\") != \"public\":\n",
    "            continue\n",
    "        snippet = v.get(\"snippet\", {})\n",
    "        vid_id = v.get(\"id\")\n",
    "        if not isinstance(vid_id, str):  # extra safety\n",
    "            continue\n",
    "        videos.append(f\"{snippet.get('title')} - https://www.youtube.com/watch?v={vid_id}\")\n",
    "        if len(videos) >= max_results:\n",
    "            break\n",
    "\n",
    "    return videos\n",
    "videos = youtube_search_guaranteed(\"difference between CNN and RNN\")\n",
    "for v in videos:\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7836f347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fcd1ce81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUERY: Explain attention mechanism in transformers and show me related videos\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY: Create a resume for a software engineer position; Also explain what skills are most important for this role\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY: What are the latest AI trends today?\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY: Summarize this video https://www.youtube.com/watch?v=example and explain the key concepts\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY: Find research papers about quantum computing and explain the basic concepts\n",
      "============================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Annotated, TypedDict, Literal\n",
    "import re\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.utilities.semanticscholar import SemanticScholarAPIWrapper\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled, VideoUnavailable\n",
    "from langchain_tavily import TavilySearch\n",
    "import operator\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Initialize LLM (You need to configure this)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# hf_model = ChatHuggingFace(...)  # Configure your model here\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# State Definition for Multi-Agent System\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    current_query: str\n",
    "    sub_queries: List[str]\n",
    "    agent_responses: Dict[str, str]\n",
    "    next_agent: str\n",
    "    final_response: str\n",
    "    context: Dict[str, any]\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Enhanced Tools\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "API_KEY = \"AIzaSyBNBTgze_5FR5VHzfZlxc38iLwr7xyYaHE\"\n",
    "\n",
    "@tool\n",
    "def youtube_search(query: str) -> str:\n",
    "    \"\"\"Search YouTube for videos related to the query and return video links with titles.\"\"\"\n",
    "    try:\n",
    "        url = f\"https://www.googleapis.com/youtube/v3/search\"\n",
    "        params = {\n",
    "            \"part\": \"snippet\",\n",
    "            \"q\": query,\n",
    "            \"type\": \"video\",\n",
    "            \"maxResults\": 5,\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "        res = requests.get(url, params=params).json()\n",
    "        \n",
    "        videos = []\n",
    "        for item in res.get(\"items\", []):\n",
    "            title = item['snippet']['title']\n",
    "            video_id = item['id']['videoId']\n",
    "            url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "            videos.append(f\"**{title}**: {url}\")\n",
    "        \n",
    "        return \"\\n\".join(videos) if videos else \"No related videos found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error searching YouTube: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def topic_explanation(query: str) -> str:\n",
    "    \"\"\"Return a comprehensive conceptual explanation of the topic.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Provide a detailed explanation of '{query}' covering:\n",
    "    1. Core concept and definition\n",
    "    2. Key components or principles\n",
    "    3. Real-world applications\n",
    "    4. Why it's important\n",
    "    \n",
    "    Make it beginner-friendly but comprehensive.\n",
    "    \"\"\"\n",
    "    return hf_model.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "@tool\n",
    "def generate_resume(job_description: str, candidate_info: str = \"\") -> str:\n",
    "    \"\"\"Generate a professional LaTeX resume optimized for ATS systems.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Create a modern, ATS-friendly LaTeX resume using clean formatting.\n",
    "    \n",
    "    Requirements:\n",
    "    - Use standard LaTeX packages (no exotic dependencies)\n",
    "    - Include proper sections: Contact, Summary, Skills, Experience, Education, Projects\n",
    "    - Optimize for keyword matching\n",
    "    - Professional formatting with clear hierarchy\n",
    "    - Include quantifiable achievements where possible\n",
    "    \n",
    "    Job Description: {job_description}\n",
    "    Candidate Info: {candidate_info if candidate_info else \"Entry-level candidate\"}\n",
    "    \n",
    "    Return only valid LaTeX code.\n",
    "    \"\"\"\n",
    "    return hf_model.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "# Initialize other tools\n",
    "ss = SemanticScholarAPIWrapper(top_k_results=5, load_max_docs=5)\n",
    "\n",
    "@tool\n",
    "def semantic_scholar_research(query: str) -> List[Dict]:\n",
    "    \"\"\"Fetch and summarize top research papers from Semantic Scholar.\"\"\"\n",
    "    try:\n",
    "        raw = ss.run(query)\n",
    "        papers = raw.split(\"\\n\\n\")\n",
    "        result = []\n",
    "        \n",
    "        for p in papers[:3]:  # Limit to top 3 papers\n",
    "            if \"abstract:\" in p.lower():\n",
    "                summary = hf_model.invoke([\n",
    "                    HumanMessage(content=f\"Summarize this research paper in 3-4 sentences:\\n{p}\")\n",
    "                ]).content\n",
    "                result.append({\"raw_info\": p, \"summary\": summary})\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return [{\"error\": f\"Error fetching papers: {str(e)}\"}]\n",
    "\n",
    "def get_transcript(video_id: str):\n",
    "    \"\"\"Extract transcript from YouTube video.\"\"\"\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi().fetch(video_id, languages=[\"en\", \"hi\"])\n",
    "        texts = [snippet.get(\"text\", \"\") for snippet in transcript_list]\n",
    "        return \" \".join(texts), None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "@tool\n",
    "def youtube_qa(video_url: str, question: str) -> str:\n",
    "    \"\"\"Answer questions based on YouTube video transcript.\"\"\"\n",
    "    try:\n",
    "        vid_id = video_url.split(\"v=\")[-1].split(\"&\")[0]\n",
    "        transcript, error = get_transcript(vid_id)\n",
    "        \n",
    "        if error:\n",
    "            return f\"Error getting transcript: {error}\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Based on this video transcript, answer the question:\n",
    "        \n",
    "        Transcript: {transcript[:3000]}...  # Limit context\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Provide a detailed answer based on the video content.\n",
    "        \"\"\"\n",
    "        \n",
    "        return hf_model.invoke([HumanMessage(content=prompt)]).content\n",
    "    except Exception as e:\n",
    "        return f\"Error processing video: {str(e)}\"\n",
    "\n",
    "# Initialize search tools\n",
    "duck_tool = DuckDuckGoSearchRun()\n",
    "tavily_tool = TavilySearch()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Specialized Agents\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class EducationAgent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.tools = [topic_explanation, youtube_search]\n",
    "        \n",
    "    def process(self, query: str) -> str:\n",
    "        \"\"\"Handle educational queries with explanations and video resources.\"\"\"\n",
    "        explanation = topic_explanation.invoke({\"query\": query})\n",
    "        videos = youtube_search.invoke({\"query\": query + \" tutorial explanation\"})\n",
    "        \n",
    "        response = f\"\"\"\n",
    "## Educational Response for: {query}\n",
    "\n",
    "### Conceptual Explanation:\n",
    "{explanation}\n",
    "\n",
    "### Related Video Resources:\n",
    "{videos}\n",
    "\n",
    "### Study Suggestions:\n",
    "- Start with the conceptual understanding above\n",
    "- Watch the recommended videos for visual learning\n",
    "- Practice with hands-on examples if applicable\n",
    "\"\"\"\n",
    "        return response\n",
    "\n",
    "class ResearchAgent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.tools = [semantic_scholar_research, youtube_search]\n",
    "        \n",
    "    def process(self, query: str) -> str:\n",
    "        \"\"\"Handle research queries with academic papers and videos.\"\"\"\n",
    "        papers = semantic_scholar_research.invoke({\"query\": query})\n",
    "        videos = youtube_search.invoke({\"query\": query + \" research paper explanation\"})\n",
    "        \n",
    "        response = f\"## Research Analysis for: {query}\\n\\n\"\n",
    "        \n",
    "        if papers:\n",
    "            response += \"### Academic Papers:\\n\"\n",
    "            for i, paper in enumerate(papers, 1):\n",
    "                if \"summary\" in paper:\n",
    "                    response += f\"{i}. **Summary**: {paper['summary']}\\n\\n\"\n",
    "        \n",
    "        response += f\"### Educational Videos:\\n{videos}\\n\"\n",
    "        return response\n",
    "\n",
    "class ResumeAgent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.tools = [generate_resume]\n",
    "        \n",
    "    def process(self, query: str, job_desc: str = \"\", candidate_info: str = \"\") -> str:\n",
    "        \"\"\"Handle resume creation and optimization.\"\"\"\n",
    "        resume_latex = generate_resume.invoke({\n",
    "            \"job_description\": job_desc or query,\n",
    "            \"candidate_info\": candidate_info\n",
    "        })\n",
    "        \n",
    "        return f\"\"\"\n",
    "## Resume Generation Complete\n",
    "\n",
    "### LaTeX Resume Code:\n",
    "```latex\n",
    "{resume_latex}\n",
    "```\n",
    "\n",
    "### Instructions:\n",
    "1. Copy the LaTeX code above\n",
    "2. Paste into Overleaf or any LaTeX editor\n",
    "3. Compile to generate PDF\n",
    "4. Customize further as needed\n",
    "\n",
    "### Tips:\n",
    "- Ensure all information is accurate\n",
    "- Tailor keywords to match job description\n",
    "- Proofread before submitting\n",
    "\"\"\"\n",
    "\n",
    "class NewsAgent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.tools = [tavily_tool, duck_tool]\n",
    "        \n",
    "    def process(self, query: str) -> str:\n",
    "        \"\"\"Handle news and current affairs queries.\"\"\"\n",
    "        try:\n",
    "            news_results = tavily_tool.invoke({\"query\": query + \" latest news today\"})\n",
    "            summary_prompt = f\"\"\"\n",
    "            Summarize the latest news about '{query}' based on this information:\n",
    "            {news_results}\n",
    "            \n",
    "            Provide:\n",
    "            1. Key headlines\n",
    "            2. Important developments\n",
    "            3. Implications or analysis\n",
    "            \"\"\"\n",
    "            \n",
    "            summary = self.model.invoke([HumanMessage(content=summary_prompt)]).content\n",
    "            return f\"## Latest News: {query}\\n\\n{summary}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error fetching news: {str(e)}\"\n",
    "\n",
    "class VideoAnalysisAgent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.tools = [youtube_qa, youtube_search]\n",
    "        \n",
    "    def process(self, query: str, video_url: str = None) -> str:\n",
    "        \"\"\"Handle video-specific queries.\"\"\"\n",
    "        if video_url:\n",
    "            # Direct video analysis\n",
    "            analysis = youtube_qa.invoke({\"video_url\": video_url, \"question\": query})\n",
    "            return f\"## Video Analysis\\n\\n**Question**: {query}\\n\\n**Answer**: {analysis}\"\n",
    "        else:\n",
    "            # Search for relevant videos\n",
    "            videos = youtube_search.invoke({\"query\": query})\n",
    "            return f\"## Video Resources for: {query}\\n\\n{videos}\"\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Query Analysis and Routing System\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class QueryAnalyzer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def analyze_intent(self, query: str) -> Dict[str, any]:\n",
    "        \"\"\"Use LLM to analyze query intent and extract components.\"\"\"\n",
    "        analysis_prompt = f\"\"\"\n",
    "        Analyze this user query and respond with a JSON-like structure:\n",
    "        \n",
    "        Query: \"{query}\"\n",
    "        \n",
    "        Determine:\n",
    "        1. Primary intent (education, research, resume, news, video_analysis, general)\n",
    "        2. Sub-queries if it's a multi-part query\n",
    "        3. Specific requirements (video_url if present, job_description, etc.)\n",
    "        4. Complexity level (simple, medium, complex)\n",
    "        \n",
    "        Format your response as:\n",
    "        Intent: [primary intent]\n",
    "        Sub_queries: [list of sub-queries if multi-part, otherwise just the main query]\n",
    "        Requirements: [any specific requirements]\n",
    "        Complexity: [complexity level]\n",
    "        Video_URL: [if YouTube URL present, extract it]\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.model.invoke([HumanMessage(content=analysis_prompt)]).content\n",
    "        return self._parse_analysis(response, query)\n",
    "    \n",
    "    def _parse_analysis(self, response: str, original_query: str) -> Dict[str, any]:\n",
    "        \"\"\"Parse the LLM analysis response.\"\"\"\n",
    "        intent_patterns = {\n",
    "            \"education\": [\"explain\", \"tutorial\", \"learn\", \"teach\", \"concept\", \"difference\"],\n",
    "            \"research\": [\"research\", \"paper\", \"academic\", \"study\", \"journal\"],\n",
    "            \"resume\": [\"resume\", \"cv\", \"job\", \"application\"],\n",
    "            \"news\": [\"news\", \"latest\", \"current\", \"trend\", \"update\"],\n",
    "            \"video_analysis\": [\"video\", \"youtube\", \"watch\", \"summarize video\"]\n",
    "        }\n",
    "        \n",
    "        # Extract intent from response or fallback to pattern matching\n",
    "        intent = \"general\"\n",
    "        for key, patterns in intent_patterns.items():\n",
    "            if any(pattern in response.lower() or pattern in original_query.lower() for pattern in patterns):\n",
    "                intent = key\n",
    "                break\n",
    "        \n",
    "        # Extract video URL if present\n",
    "        video_url = None\n",
    "        video_match = re.search(r'https?://(?:www\\.)?youtube\\.com/watch\\?v=([^&\\s]+)', original_query)\n",
    "        if video_match:\n",
    "            video_url = video_match.group(0)\n",
    "            intent = \"video_analysis\"\n",
    "        \n",
    "        # Split multi-part queries\n",
    "        sub_queries = re.split(r'[;|\\n]|and also|also', original_query)\n",
    "        sub_queries = [q.strip() for q in sub_queries if q.strip()]\n",
    "        \n",
    "        return {\n",
    "            \"intent\": intent,\n",
    "            \"sub_queries\": sub_queries if len(sub_queries) > 1 else [original_query],\n",
    "            \"video_url\": video_url,\n",
    "            \"complexity\": \"complex\" if len(sub_queries) > 1 else \"simple\"\n",
    "        }\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Hierarchical Supervisor System\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class MultiAgentSupervisor:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.analyzer = QueryAnalyzer(model)\n",
    "        \n",
    "        # Initialize specialized agents\n",
    "        self.agents = {\n",
    "            \"education\": EducationAgent(model),\n",
    "            \"research\": ResearchAgent(model),\n",
    "            \"resume\": ResumeAgent(model),\n",
    "            \"news\": NewsAgent(model),\n",
    "            \"video_analysis\": VideoAnalysisAgent(model)\n",
    "        }\n",
    "        \n",
    "    def process_query(self, query: str) -> str:\n",
    "        \"\"\"Main entry point for processing any query.\"\"\"\n",
    "        # Analyze the query\n",
    "        analysis = self.analyzer.analyze_intent(query)\n",
    "        \n",
    "        # Handle multi-part queries\n",
    "        if len(analysis[\"sub_queries\"]) > 1:\n",
    "            return self._process_multi_query(analysis)\n",
    "        else:\n",
    "            return self._process_single_query(analysis)\n",
    "    \n",
    "    def _process_single_query(self, analysis: Dict) -> str:\n",
    "        \"\"\"Process a single query.\"\"\"\n",
    "        intent = analysis[\"intent\"]\n",
    "        query = analysis[\"sub_queries\"][0]\n",
    "        \n",
    "        if intent in self.agents:\n",
    "            agent = self.agents[intent]\n",
    "            if intent == \"video_analysis\" and analysis.get(\"video_url\"):\n",
    "                return agent.process(query, analysis[\"video_url\"])\n",
    "            else:\n",
    "                return agent.process(query)\n",
    "        else:\n",
    "            # Fallback to general response\n",
    "            return self._general_response(query)\n",
    "    \n",
    "    def _process_multi_query(self, analysis: Dict) -> str:\n",
    "        \"\"\"Process multiple sub-queries.\"\"\"\n",
    "        responses = []\n",
    "        \n",
    "        for i, sub_query in enumerate(analysis[\"sub_queries\"], 1):\n",
    "            # Re-analyze each sub-query\n",
    "            sub_analysis = self.analyzer.analyze_intent(sub_query)\n",
    "            response = self._process_single_query(sub_analysis)\n",
    "            \n",
    "            responses.append(f\"## Part {i}: {sub_query}\\n{response}\\n\")\n",
    "        \n",
    "        final_response = \"# Multi-Part Query Response\\n\\n\" + \"\\n---\\n\\n\".join(responses)\n",
    "        \n",
    "        # Add synthesis if needed\n",
    "        if len(responses) > 2:\n",
    "            synthesis = self._synthesize_responses(analysis[\"sub_queries\"], responses)\n",
    "            final_response += f\"\\n---\\n\\n## Synthesis\\n{synthesis}\"\n",
    "        \n",
    "        return final_response\n",
    "    \n",
    "    def _synthesize_responses(self, queries: List[str], responses: List[str]) -> str:\n",
    "        \"\"\"Synthesize multiple responses into a coherent summary.\"\"\"\n",
    "        synthesis_prompt = f\"\"\"\n",
    "        The user asked multiple related questions:\n",
    "        {queries}\n",
    "        \n",
    "        Provide a brief synthesis that connects these topics and highlights:\n",
    "        1. Common themes\n",
    "        2. How these topics relate to each other\n",
    "        3. Key takeaways\n",
    "        \n",
    "        Keep it concise but insightful.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.model.invoke([HumanMessage(content=synthesis_prompt)]).content\n",
    "    \n",
    "    def _general_response(self, query: str) -> str:\n",
    "        \"\"\"Fallback for general queries.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Provide a helpful response to this query: {query}\n",
    "        \n",
    "        If you need more specific information or tools, suggest what the user should specify.\n",
    "        \"\"\"\n",
    "        return self.model.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Main Interface\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class IntelligentAssistant:\n",
    "    def __init__(self, model):\n",
    "        self.supervisor = MultiAgentSupervisor(model)\n",
    "    \n",
    "    def process(self, query: str) -> str:\n",
    "        \"\"\"Main interface for processing user queries.\"\"\"\n",
    "        try:\n",
    "            response = self.supervisor.process_query(query)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"I apologize, but I encountered an error processing your request: {str(e)}\\nPlease try rephrasing your query or contact support if the issue persists.\"\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Usage Example\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the assistant (you need to configure your model)\n",
    "    # assistant = IntelligentAssistant(hf_model)\n",
    "    \n",
    "    # Example queries\n",
    "    test_queries = [\n",
    "        \"Explain attention mechanism in transformers and show me related videos\",\n",
    "        \"Create a resume for a software engineer position; Also explain what skills are most important for this role\",\n",
    "        \"What are the latest AI trends today?\",\n",
    "        \"Summarize this video https://www.youtube.com/watch?v=example and explain the key concepts\",\n",
    "        \"Find research papers about quantum computing and explain the basic concepts\"\n",
    "    ]\n",
    "    \n",
    "    # Process queries\n",
    "    for query in test_queries:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"QUERY: {query}\")\n",
    "        print('='*60)\n",
    "        # response = assistant.process(query)\n",
    "        # print(response)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c25a93be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SemanticScholarAPIWrapper(top_k_results=5, load_max_docs=5)\n",
    "def semantic_scholar_research(query: str) -> List[Dict]:\n",
    "    \"\"\"Fetch and summarize top research papers from Semantic Scholar.\"\"\"\n",
    "    try:\n",
    "        raw = ss.run(query)\n",
    "        papers = raw.split(\"\\n\\n\")\n",
    "        result = []\n",
    "        \n",
    "        for p in papers[:3]:  # Limit to top 3 papers\n",
    "            if \"abstract:\" in p.lower():\n",
    "                summary = hf_model.invoke([\n",
    "                    HumanMessage(content=f\"Summarize this research paper in 3-4 sentences:\\n{p}\")\n",
    "                ]).content\n",
    "                result.append({\"raw_info\": p, \"summary\": summary})\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return [{\"error\": f\"Error fetching papers: {str(e)}\"}]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4c5d4472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_scholar_research(\"give me the summary of top 5 rsearch ppaer on Attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b8cd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
